{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate DA for PE Score v8 (using Pandas DataFrame)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### DEVELOPMENT ONLY ### \n",
    "\n",
    "# Create a fresh copy of \"PE_Grid_calc.  This Feature Class will contain the intermediate calculations and results.\n",
    "arcpy.Delete_management(\"PE_Grid_calc\")\n",
    "arcpy.CopyFeatures_management(in_features=\"PE_Grid_clean\", out_feature_class=\"PE_Grid_calc\")\n",
    "\n",
    "# Create a fresh copy of \"PE_Grid_output.  This Feature Class will contain only the final results.\n",
    "arcpy.Delete_management(\"PE_Grid_output\")\n",
    "arcpy.CopyFeatures_management(in_features=\"PE_Grid_clean\", out_feature_class=\"PE_Grid_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE_Grid attributes: ['OBJECTID', 'Shape', 'LG_index', 'SD_index', 'LD_index', 'UD_index', 'Shape_Length', 'Shape_Area'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Create lists for unique components and each corresponding dataset \"\"\"\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL ###\n",
    "\n",
    "### COMMENT FOR DEVELOPMENT: RUN THIS CELL FOR EACH SESSION ###\n",
    "\n",
    "### THIS CELL IS NEEDED IN THE FINAL SCRIPT ###\n",
    "\n",
    "\n",
    "import arcpy\n",
    "from time import process_time\n",
    "import pandas as pd\n",
    "\n",
    "# UNTESTED: SWITCH BETWEEN DA AND DS\n",
    "FeatureDataset=\"DA\"\n",
    "\n",
    "# Identify working files and workspace on C:\\ (testing SSD vs. HDD performace) -- POWDER RIVER BASIN\n",
    "workspace_dir = r\"C:\\Users\\creasonc\\REE_PE_Script_dev\\11-1-19\"\n",
    "workspace_gdb = r\"REE_EnrichmentDatabase_PRB_DA_DS.gdb\"\n",
    "PE_Grid_file = r\"PE_Grid_calc\"\n",
    "\n",
    "# Identify working files and workspace -- POWDER RIVER BASIN\n",
    "# workspace_dir = r\"E:/REE/PE_Score_Calc/Development/10-10-19\"\n",
    "# workspace_gdb = r\"REE_EnrichmentDatabase_PRB_cgc.gdb\"\n",
    "# PE_Grid_file = r\"PE_Grid_ScriptTest_20200401\"\n",
    "\n",
    "# Identify working files and workspace -- CENTRAL APP BASIN\n",
    "# workspace_dir = r\"E:/REE/PE_Score_Calc/Development/10-10-19\"\n",
    "# workspace_gdb = r\"REE_EnrichmentDatabase_CAB_cgc.gdb\"\n",
    "# PE_Grid_file = r\"PE_Grid_clean\"\n",
    "\n",
    "workspace = workspace_dir + \"/\" + workspace_gdb\n",
    "\n",
    "# Set ArcGIS workspace environment\n",
    "arcpy.env.workspace = workspace\n",
    "PE_Grid = workspace + \"/\" + PE_Grid_file\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "def ListFeatureClasses(WildCard, FeatureDataset, fullname, first_char, last_char):\n",
    "    \"\"\"\n",
    "    Function that creates a list of all unique REE-Coal components in an ESRI GDB Feature Dataset, for use in use in \n",
    "        calculating PE score from DA and DS databases.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    WildCard: <str>\n",
    "        Criteria used to limit the results returned\n",
    "    FeatureDataset: <str>\n",
    "        Name of the feature dataset \n",
    "    fullname: <str>\n",
    "        Yes or no to return the full filenames.  Yes must be entered as 'y' or 'yes'.\n",
    "    first_char: <str>\n",
    "        Index of first character to include in the filename   \n",
    "    last_char: <str>\n",
    "        Index of last character to include in the filename\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    <list>\n",
    "        sorted, non-repeating iterable sequence of feature class names based on the WildCard criteria\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_list = arcpy.ListFeatureClasses(WildCard, feature_dataset=FeatureDataset)  # list all feature classes\n",
    "\n",
    "    fc_names = []  # create empty \n",
    "\n",
    "    # extract code prefixes from all feature classes\n",
    "    if fullname == 'y' or fullname == 'yes':    \n",
    "        for features in feature_list:\n",
    "            fc_names.append(features)  # extract all characters in the filename\n",
    "    else:\n",
    "        for features in feature_list:\n",
    "            fc_names.append(features[first_char:last_char])  # extract only a portion of the filename\n",
    "        \n",
    "    fc_names = list(set(fc_names))  # sort and filter out repeated values, convert to list from dictionary\n",
    "    \n",
    "    return sorted(fc_names)\n",
    "\n",
    "\n",
    "def ListFieldNames(featureclass):\n",
    "    \"\"\"\n",
    "    Lists the fields in a feature class, shapefile, or table in a specified dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    featureclass: <str>\n",
    "        Name of feature class\n",
    "        \n",
    "    Can modify to include: wild_card, field_type in arcpy.ListFields()\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    <list>\n",
    "        Field names \n",
    "    \"\"\"\n",
    "    fc_wksp = arcpy.env.workspace + \"/\" + featureclass\n",
    "    \n",
    "    field_names = [f.name for f in arcpy.ListFields(featureclass)]\n",
    "    \n",
    "    return field_names\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# Create a list of all unique code prefixes for the component IDs\n",
    "unique_components = ListFeatureClasses(WildCard=\"DA*\", FeatureDataset=\"DA\", fullname='no', first_char=0, last_char=14)\n",
    "\n",
    "# An array comprising all components and their respective feature classes\n",
    "components_data_array = []\n",
    "\n",
    "# Generate a list of feature classes for each Emplacement Type, Influence Extent, AND Component ID combination\n",
    "for component_datasets in unique_components:\n",
    "#     print(\"component_datasets:\", component_datasets, \"\\n\")\n",
    "    component_datasets = ListFeatureClasses(WildCard=(component_datasets + \"*\"), FeatureDataset=\"DA\", fullname='yes', first_char=0, last_char=8)\n",
    "#     print(\"component_datasets:\", component_datasets, \"\\n\")\n",
    "    # Append list to a single array\n",
    "    components_data_array.append(component_datasets)\n",
    "    \n",
    "# del(component_datasets)\n",
    "\n",
    "# List field names \n",
    "field_names = ListFieldNames(PE_Grid)\n",
    "\n",
    "print(\"PE_Grid attributes:\", field_names, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DA_Eo_LD_CID10', 'DA_Eo_LD_CID16', 'DA_Fl_LD_CID01', 'DA_Fl_LD_CID02', 'DA_Fl_LD_CID03', 'DA_Fl_LD_CID04', 'DA_Fl_LD_CID05', 'DA_Fl_LD_CID06', 'DA_Fl_LD_CID07', 'DA_Fl_LD_CID08', 'DA_Fl_LD_CID09', 'DA_Fl_LD_CID10', 'DA_HA_LD_CID24', 'DA_HA_LD_CID25', 'DA_HA_LD_CID26', 'DA_HA_LD_CID27', 'DA_HA_LD_CID28', 'DA_HA_LD_CID29', 'DA_HA_LD_CID30', 'DA_HA_LD_CID31', 'DA_HA_LD_CID32', 'DA_HA_LG_CID42', 'DA_HA_LG_CID47', 'DA_HA_LG_CID48', 'DA_HA_LG_CID50', 'DA_HA_LG_CID54', 'DA_HA_UD_CID37', 'DA_HA_UD_CID38', 'DA_HA_UD_CID39', 'DA_HA_UD_CID40', 'DA_HA_UD_CID43', 'DA_HP_LD_CID24', 'DA_HP_LD_CID25', 'DA_HP_LD_CID26', 'DA_HP_LD_CID27', 'DA_HP_LD_CID28', 'DA_HP_LD_CID29', 'DA_HP_LD_CID30', 'DA_HP_LD_CID31', 'DA_HP_LD_CID32', 'DA_HP_LG_CID42', 'DA_HP_LG_CID46', 'DA_HP_LG_CID50', 'DA_HP_LG_CID56', 'DA_HP_LG_CID57', 'DA_HP_UD_CID37', 'DA_HP_UD_CID38', 'DA_HP_UD_CID39', 'DA_HP_UD_CID40', 'DA_HP_UD_CID43', 'DA_HP_UD_CID45', 'DA_MA_LD_CID24', 'DA_MA_LD_CID25', 'DA_MA_LD_CID26', 'DA_MA_LD_CID27', 'DA_MA_LD_CID28', 'DA_MA_LD_CID29', 'DA_MA_LD_CID30', 'DA_MA_LD_CID31', 'DA_MA_LD_CID32', 'DA_MA_LG_CID42', 'DA_MA_LG_CID47', 'DA_MA_LG_CID48', 'DA_MA_LG_CID50', 'DA_MA_LG_CID54', 'DA_MA_UD_CID37', 'DA_MA_UD_CID38', 'DA_MA_UD_CID39', 'DA_MA_UD_CID40', 'DA_MA_UD_CID43', 'DA_MP_LD_CID24', 'DA_MP_LD_CID25', 'DA_MP_LD_CID26', 'DA_MP_LD_CID27', 'DA_MP_LD_CID28', 'DA_MP_LD_CID29', 'DA_MP_LD_CID30', 'DA_MP_LD_CID31', 'DA_MP_LD_CID32', 'DA_MP_LG_CID42', 'DA_MP_LG_CID50', 'DA_MP_LG_CID56', 'DA_MP_LG_CID57', 'DA_MP_UD_CID37', 'DA_MP_UD_CID38', 'DA_MP_UD_CID39', 'DA_MP_UD_CID40', 'DA_MP_UD_CID43']\n"
     ]
    }
   ],
   "source": [
    "FeatureDataset=\"DA\"\n",
    "a = ListFeatureClasses(WildCard=FeatureDataset+\"*\", FeatureDataset=FeatureDataset, fullname='no', first_char=0, last_char=14)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List field names \n",
    "field_names = ListFieldNames(PE_Grid)\n",
    "print(\"PE_Grid attributes:\", field_names, \"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Delete feature class fields from PE_Grid \"\"\"\n",
    "\n",
    "### DEVELOPMENT ONLY ###\n",
    "\n",
    "field_names = ListFieldNames(PE_Grid)\n",
    "field_names\n",
    "\n",
    "remove_fields = [i for i in field_names if \"_domain_\" in i]\n",
    "remove_fields\n",
    "\n",
    "for field in remove_fields:\n",
    "    arcpy.DeleteField_management(in_table=PE_Grid, drop_field=field)\n",
    "\n",
    "field_names = ListFieldNames(PE_Grid)\n",
    "field_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DA_Eo_LD_CID10_SGMC_Geology time: 399.98 seconds\n",
      "DA_Eo_LD_CID10_USTRAT time: 209.0 seconds\n",
      "DA_Eo_LD_CID16_USTRAT time: 139.48 seconds\n",
      "DA_Fl_LD_CID01_deposit time: 119.42 seconds\n",
      "DA_Fl_LD_CID01_mrds_trim time: 124.91 seconds\n",
      "DA_Fl_LD_CID02_main time: 118.66 seconds\n",
      "DA_Fl_LD_CID02_mrds_trim time: 125.2 seconds\n",
      "DA_Fl_LD_CID03_mrds_trim time: 125.38 seconds\n",
      "DA_Fl_LD_CID04_mrds_trim time: 124.3 seconds\n",
      "DA_Fl_LD_CID05_mrds_trim time: 197.22 seconds\n",
      "DA_Fl_LD_CID06_mrds_trim time: 126.36 seconds\n",
      "DA_Fl_LD_CID07_mrds_trim time: 125.98 seconds\n",
      "DA_Fl_LD_CID08_deposit time: 119.89 seconds\n",
      "DA_Fl_LD_CID08_mrds_trim time: 125.89 seconds\n",
      "DA_Fl_LD_CID09_mrds_trim time: 126.3 seconds\n",
      "DA_Fl_LD_CID10_SGMC_Geology time: 388.2 seconds\n",
      "DA_Fl_LD_CID10_USTRAT time: 140.72 seconds\n",
      "DA_HA_LD_CID24_deposit time: 191.64 seconds\n",
      "DA_HA_LD_CID24_mrds_trim time: 127.03 seconds\n",
      "DA_HA_LD_CID25_main time: 120.33 seconds\n",
      "DA_HA_LD_CID25_mrds_trim time: 126.77 seconds\n",
      "DA_HA_LD_CID26_mrds_trim time: 127.36 seconds\n",
      "DA_HA_LD_CID27_mrds_trim time: 127.67 seconds\n",
      "DA_HA_LD_CID28_mrds_trim time: 127.86 seconds\n",
      "DA_HA_LD_CID29_mrds_trim time: 128.7 seconds\n",
      "DA_HA_LD_CID30_mrds_trim time: 201.17 seconds\n",
      "DA_HA_LD_CID31_mrds_trim time: 131.02 seconds\n",
      "DA_HA_LD_CID32_mrds_trim time: 132.89 seconds\n",
      "DA_HA_LG_CID42_PRBdrillholes time: 138.12 seconds\n",
      "DA_HA_LG_CID42_USTRAT time: 147.31 seconds\n",
      "DA_HA_LG_CID47_COALQUAL_Proximate_Ultimate time: 129.81 seconds\n",
      "DA_HA_LG_CID48_SGMC_Geology time: 406.0 seconds\n",
      "DA_HA_LG_CID48_USTRAT time: 147.62 seconds\n",
      "DA_HA_LG_CID50_USGSPWDBv2_3n time: 208.56 seconds\n",
      "DA_HA_LG_CID50_USTRAT time: 145.22 seconds\n",
      "DA_HA_LG_CID54_USTRAT time: 148.67 seconds\n",
      "DA_HA_UD_CID37_USTRAT time: 148.7 seconds\n",
      "DA_HA_UD_CID38_PRBdrillholes time: 139.62 seconds\n",
      "DA_HA_UD_CID38_SGMC_Geology time: 411.61 seconds\n",
      "DA_HA_UD_CID39_PRBdrillholes time: 139.89 seconds\n",
      "DA_HA_UD_CID39_USTRAT time: 149.11 seconds\n",
      "DA_HA_UD_CID40_SGMC_Geology time: 487.2 seconds\n",
      "DA_HA_UD_CID40_USTRAT time: 149.12 seconds\n",
      "DA_HA_UD_CID43_Faults time: 134.78 seconds\n",
      "DA_HA_UD_CID43_MTfaults_dd time: 128.55 seconds\n",
      "DA_HA_UD_CID43_SGMC_Structure time: 144.66 seconds\n",
      "DA_HA_UD_CID43_USTRAT time: 149.95 seconds\n",
      "DA_HA_UD_CID43_qfaults time: 125.78 seconds\n",
      "DA_HP_LD_CID24_deposit time: 126.02 seconds\n",
      "DA_HP_LD_CID24_mrds_trim time: 208.22 seconds\n",
      "DA_HP_LD_CID25_main time: 125.66 seconds\n",
      "DA_HP_LD_CID25_mrds_trim time: 135.34 seconds\n",
      "DA_HP_LD_CID26_mrds_trim time: 136.52 seconds\n",
      "DA_HP_LD_CID27_mrds_trim time: 135.5 seconds\n",
      "DA_HP_LD_CID28_mrds_trim time: 135.19 seconds\n",
      "DA_HP_LD_CID29_mrds_trim time: 135.64 seconds\n",
      "DA_HP_LD_CID30_mrds_trim time: 135.62 seconds\n",
      "DA_HP_LD_CID31_mrds_trim time: 210.02 seconds\n",
      "DA_HP_LD_CID32_mrds_trim time: 136.11 seconds\n",
      "DA_HP_LG_CID42_PRBdrillholes time: 134.91 seconds\n",
      "DA_HP_LG_CID42_USTRAT time: 153.88 seconds\n",
      "DA_HP_LG_CID46_COALQUAL_Trace_Elements time: 134.25 seconds\n",
      "DA_HP_LG_CID46_USTRAT time: 152.92 seconds\n",
      "DA_HP_LG_CID46_mrds_trim time: 137.0 seconds\n",
      "DA_HP_LG_CID50_USGSPWDBv2_3n time: 138.78 seconds\n",
      "DA_HP_LG_CID50_USTRAT time: 225.81 seconds\n",
      "DA_HP_LG_CID56_USTRAT time: 154.48 seconds\n",
      "DA_HP_LG_CID57_COALQUAL_Trace_Elements time: 135.33 seconds\n",
      "DA_HP_LG_CID57_USTRAT time: 152.03 seconds\n",
      "DA_HP_UD_CID37_USTRAT time: 153.89 seconds\n",
      "DA_HP_UD_CID38_PRBdrillholes time: 143.88 seconds\n",
      "DA_HP_UD_CID38_SGMC_Geology time: 426.88 seconds\n",
      "DA_HP_UD_CID39_PRBdrillholes time: 143.44 seconds\n",
      "DA_HP_UD_CID39_USTRAT time: 230.7 seconds\n",
      "DA_HP_UD_CID40_SGMC_Geology time: 427.59 seconds\n",
      "DA_HP_UD_CID40_USTRAT time: 155.25 seconds\n",
      "DA_HP_UD_CID43_Faults time: 139.25 seconds\n",
      "DA_HP_UD_CID43_MTfaults_dd time: 132.17 seconds\n",
      "DA_HP_UD_CID43_SGMC_Structure time: 150.5 seconds\n",
      "DA_HP_UD_CID43_USTRAT time: 156.44 seconds\n",
      "DA_HP_UD_CID43_qfaults time: 129.36 seconds\n",
      "DA_HP_UD_CID45_SGMC_Geology time: 504.59 seconds\n",
      "DA_HP_UD_CID45_USTRAT time: 156.2 seconds\n",
      "DA_MA_LD_CID24_deposit time: 130.52 seconds\n",
      "DA_MA_LD_CID24_mrds_trim time: 140.56 seconds\n",
      "DA_MA_LD_CID25_main time: 130.39 seconds\n",
      "DA_MA_LD_CID25_mrds_trim time: 138.3 seconds\n",
      "DA_MA_LD_CID26_mrds_trim time: 137.47 seconds\n",
      "DA_MA_LD_CID27_mrds_trim time: 137.47 seconds\n",
      "DA_MA_LD_CID28_mrds_trim time: 209.66 seconds\n",
      "DA_MA_LD_CID29_mrds_trim time: 138.38 seconds\n",
      "DA_MA_LD_CID30_mrds_trim time: 138.25 seconds\n",
      "DA_MA_LD_CID31_mrds_trim time: 138.78 seconds\n",
      "DA_MA_LD_CID32_mrds_trim time: 139.05 seconds\n",
      "DA_MA_LG_CID42_PRBdrillholes time: 144.53 seconds\n",
      "DA_MA_LG_CID42_USTRAT time: 156.73 seconds\n",
      "DA_MA_LG_CID47_COALQUAL_Proximate_Ultimate time: 136.64 seconds\n",
      "DA_MA_LG_CID48_SGMC_Geology time: 502.02 seconds\n",
      "DA_MA_LG_CID48_USTRAT time: 156.19 seconds\n",
      "DA_MA_LG_CID50_USGSPWDBv2_3n time: 142.45 seconds\n",
      "DA_MA_LG_CID50_USTRAT time: 156.03 seconds\n",
      "DA_MA_LG_CID54_USTRAT time: 158.97 seconds\n",
      "DA_MA_UD_CID37_USTRAT time: 162.22 seconds\n",
      "DA_MA_UD_CID38_PRBdrillholes time: 147.14 seconds\n",
      "DA_MA_UD_CID38_SGMC_Geology time: 429.59 seconds\n",
      "DA_MA_UD_CID39_PRBdrillholes time: 219.72 seconds\n",
      "DA_MA_UD_CID39_USTRAT time: 156.58 seconds\n",
      "DA_MA_UD_CID40_SGMC_Geology time: 433.89 seconds\n",
      "DA_MA_UD_CID40_USTRAT time: 157.98 seconds\n",
      "DA_MA_UD_CID43_Faults time: 139.92 seconds\n",
      "DA_MA_UD_CID43_MTfaults_dd time: 135.03 seconds\n",
      "DA_MA_UD_CID43_SGMC_Structure time: 154.33 seconds\n",
      "DA_MA_UD_CID43_USTRAT time: 162.5 seconds\n",
      "DA_MA_UD_CID43_qfaults time: 205.47 seconds\n",
      "DA_MP_LD_CID24_deposit time: 129.16 seconds\n",
      "DA_MP_LD_CID24_mrds_trim time: 141.61 seconds\n",
      "DA_MP_LD_CID25_main time: 127.23 seconds\n",
      "DA_MP_LD_CID25_mrds_trim time: 121.78 seconds\n",
      "DA_MP_LD_CID26_mrds_trim time: 125.91 seconds\n",
      "DA_MP_LD_CID27_mrds_trim time: 142.14 seconds\n",
      "DA_MP_LD_CID28_mrds_trim time: 135.73 seconds\n",
      "DA_MP_LD_CID29_mrds_trim time: 193.72 seconds\n",
      "DA_MP_LD_CID30_mrds_trim time: 123.25 seconds\n",
      "DA_MP_LD_CID31_mrds_trim time: 123.14 seconds\n",
      "DA_MP_LD_CID32_mrds_trim time: 124.25 seconds\n",
      "DA_MP_LG_CID42_PRBdrillholes time: 127.98 seconds\n",
      "DA_MP_LG_CID42_USTRAT time: 152.8 seconds\n",
      "DA_MP_LG_CID50_USGSPWDBv2_3n time: 147.27 seconds\n",
      "DA_MP_LG_CID50_USTRAT time: 138.3 seconds\n",
      "DA_MP_LG_CID56_USTRAT time: 200.03 seconds\n",
      "DA_MP_LG_CID57_COALQUAL_Trace_Elements time: 142.7 seconds\n",
      "DA_MP_LG_CID57_USTRAT time: 157.8 seconds\n",
      "DA_MP_UD_CID37_USTRAT time: 144.66 seconds\n",
      "DA_MP_UD_CID38_PRBdrillholes time: 127.12 seconds\n",
      "DA_MP_UD_CID38_SGMC_Geology time: 430.05 seconds\n",
      "DA_MP_UD_CID39_PRBdrillholes time: 132.12 seconds\n",
      "DA_MP_UD_CID39_USTRAT time: 139.45 seconds\n",
      "DA_MP_UD_CID40_SGMC_Geology time: 490.23 seconds\n",
      "DA_MP_UD_CID40_USTRAT time: 160.42 seconds\n",
      "DA_MP_UD_CID43_Faults time: 130.16 seconds\n",
      "DA_MP_UD_CID43_MTfaults_dd time: 132.47 seconds\n",
      "DA_MP_UD_CID43_SGMC_Structure time: 134.05 seconds\n",
      "DA_MP_UD_CID43_USTRAT time: 165.33 seconds\n",
      "DA_MP_UD_CID43_qfaults time: 134.53 seconds\n",
      "Runtime: 24861.27 seconds\n",
      "Runtime: 414.35 minutes\n",
      "Runtime: 6.91 hours\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Calculate DA step 1 of 4: Presence/absence for each feature class in the DA Feature Dataset.  \n",
    "    Creates a new field in PE_Grid for each feature class in the geodatabase \"\"\"\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL ###  \n",
    "\n",
    "\"\"\" THIS CELL ONLY NEEDS EXECUTED ONCE (TAKES ~2.5 HOURS TO EXECUTE) \"\"\"\n",
    "\n",
    "### THIS CELL IS NEEDED IN THE FINAL SCRIPT ###\n",
    "\n",
    "\n",
    "from time import process_time\n",
    "\n",
    "######################################################################################################################\n",
    "def replaceNULL(feature_class, field):\n",
    "    \"\"\"\n",
    "    Replace NULL values with zeros for a field in a feature class\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_class: <str>\n",
    "        Name of feature class containing the field to be modified\n",
    "    field: <str>\n",
    "        Name of the field to be evaluated and modified if necessary\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    None; this function only modifies the field in the feature_class\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create update cursor for feature class \n",
    "    with arcpy.da.UpdateCursor(feature_class, field) as cursor:\n",
    "        # For each row, evaluate field value and replace with zero if NULL\n",
    "        for row in cursor:\n",
    "            if row[0] == None:\n",
    "                row[0] = 0\n",
    "            else: \n",
    "                row[0] = row[0]\n",
    "            # Update the cursor with the updated list\n",
    "            cursor.updateRow(row)\n",
    "    return\n",
    "######################################################################################################################\n",
    "\n",
    "t_start = process_time()  # track processing time\n",
    "\n",
    "processing = {}  # dictionary for processing time\n",
    "\n",
    "\n",
    "# Iterate through features for each component, add new field with the code prefix, test for intersection with PE_Grid and features, add DA\n",
    "for component_datasets in components_data_array:\n",
    "    # Test for intersect between PE_Grid cells and data features\n",
    "    for feature_class in component_datasets:            \n",
    "        \n",
    "        t1 = process_time()\n",
    "        \n",
    "#        # this variable is the component code prefix (e.g., DA_Eo_LD_CID16) at the current iteration step\n",
    "#         component = unique_components[component_datasets.index(feature_class)]\n",
    "        \n",
    "        # Create new field with same name as component code prefix and feature class\n",
    "#         arcpy.AddField_management(PE_Grid, feature_class, \"SHORT\")\n",
    "#         print(\"added field for feature_class:\", feature_class)\n",
    "        \n",
    "        # Select layer by location\n",
    "        selection = arcpy.SelectLayerByLocation_management(in_layer=PE_Grid, \n",
    "                                                           overlap_type=\"INTERSECT\", \n",
    "                                                           select_features=feature_class, \n",
    "                                                           search_distance=\"\", \n",
    "                                                           selection_type=\"NEW_SELECTION\", \n",
    "                                                           invert_spatial_relationship=\"NOT_INVERT\")\n",
    "#         print(\"selected layer for feature_class:\", feature_class)\n",
    "       \n",
    "        # Create new layer from selection\n",
    "        selection_lyr = arcpy.CopyFeatures_management(selection, feature_class + \"_selected\")\n",
    "#         print(\"copied layer for selection_lyr:\", selection_lyr)       \n",
    "       \n",
    "        # Delete from PE_Grid the field with same name as component code prefix and feature class\n",
    "#         arcpy.DeleteField_management(in_table=PE_Grid, drop_field=feature_class)\n",
    "#         print(\"deleted field for feature_class:\", feature_class)\n",
    "        \n",
    "        # Set select field to 1 \n",
    "        calc_field = arcpy.CalculateField_management(in_table=selection_lyr, \n",
    "                                                     field=feature_class, \n",
    "                                                     expression=\"1\", \n",
    "                                                     expression_type=\"PYTHON3\", \n",
    "                                                     code_block=\"\")\n",
    "#         print(\"calculated field for feature_class:\", feature_class)\n",
    "        \n",
    "        # Join field to PE_Grid (add DA_component_featureclass field from 'selection')\n",
    "        join_field = arcpy.JoinField_management(in_data=PE_Grid, \n",
    "                                                in_field=\"LG_index\", \n",
    "                                                join_table=selection_lyr, \n",
    "                                                join_field=\"LG_index\", \n",
    "                                                fields=feature_class)\n",
    "#         print(\"joined field for feature_class:\", feature_class, \"\\n\")\n",
    "        \n",
    "        # Replace Null values for the DA_featureclass field\n",
    "        replaceNULL(PE_Grid, feature_class)\n",
    "    \n",
    "        # Delete selection layer from the geodatabase\n",
    "        arcpy.Delete_management(selection_lyr)\n",
    "        \n",
    "        # print processing times for each feature class\n",
    "        t2 = process_time()\n",
    "        dt = t2-t1\n",
    "        processing[feature_class]= round(dt, 2)  # update the processing time dictionary\n",
    "        print(feature_class, \"time:\", round(dt, 2), \"seconds\")\n",
    "        \n",
    "#         break  # Development only   \n",
    "#     break  # Development only\n",
    "    \n",
    "t_stop = process_time()\n",
    "seconds = t_stop-t_start\n",
    "minutes = seconds/60\n",
    "hours = minutes/60\n",
    "\n",
    "print(\"Runtime:\", round(seconds, 2), \"seconds\")\n",
    "print(\"Runtime:\", round(minutes, 2), \"minutes\")\n",
    "print(\"Runtime:\", round(hours, 2), \"hours\")\n",
    "\n",
    "# Print processing times to csv file\n",
    "step1_time = pd.Series(processing, name='seconds')\n",
    "step1_time.to_csv(workspace_dir + '\\step1_time.csv', header=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Clean up the geodatabase \"\"\"\n",
    "\n",
    "### DEVELOPMENT ONLY ###\n",
    "\n",
    "selection_lyr\n",
    "\n",
    "# List of files in the geodatabase that are not in a feature dataset\n",
    "sel = ListFeatureClasses(WildCard=\"DA*\", FeatureDataset=\"\", fullname='yes', first_char=0, last_char=14)\n",
    "sel\n",
    "\n",
    "# List of files in sel with \"_selected\" in the filename\n",
    "# sel_files = [i for i in sel if \"_selected\" in i]\n",
    "# sel_files\n",
    "\n",
    "# for FILE in sel:\n",
    "#     arcpy.Delete_management(FILE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DEVELOPMENT ONLY ###\n",
    "\n",
    "field_names = ListFieldNames(PE_Grid)\n",
    "field_names\n",
    "# print(\"PE_Grid attributes:\", field_names, \"\\n\")\n",
    "\n",
    "# print(len(components_data_array))\n",
    "# print(len(unique_components))\n",
    "\n",
    "# print(components_data_array[0])\n",
    "# print(component_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new field: DA_Eo_LD_CID10\n",
      "Added new field: DA_Eo_LD_CID16\n",
      "Added new field: DA_Fl_LD_CID01\n",
      "Added new field: DA_Fl_LD_CID02\n",
      "Added new field: DA_Fl_LD_CID03\n",
      "Added new field: DA_Fl_LD_CID04\n",
      "Added new field: DA_Fl_LD_CID05\n",
      "Added new field: DA_Fl_LD_CID06\n",
      "Added new field: DA_Fl_LD_CID07\n",
      "Added new field: DA_Fl_LD_CID08\n",
      "Added new field: DA_Fl_LD_CID09\n",
      "Added new field: DA_Fl_LD_CID10\n",
      "Added new field: DA_HA_LD_CID24\n",
      "Added new field: DA_HA_LD_CID25\n",
      "Added new field: DA_HA_LD_CID26\n",
      "Added new field: DA_HA_LD_CID27\n",
      "Added new field: DA_HA_LD_CID28\n",
      "Added new field: DA_HA_LD_CID29\n",
      "Added new field: DA_HA_LD_CID30\n",
      "Added new field: DA_HA_LD_CID31\n",
      "Added new field: DA_HA_LD_CID32\n",
      "Added new field: DA_HA_LG_CID42\n",
      "Added new field: DA_HA_LG_CID47\n",
      "Added new field: DA_HA_LG_CID48\n",
      "Added new field: DA_HA_LG_CID50\n",
      "Added new field: DA_HA_LG_CID54\n",
      "Added new field: DA_HA_UD_CID37\n",
      "Added new field: DA_HA_UD_CID38\n",
      "Added new field: DA_HA_UD_CID39\n",
      "Added new field: DA_HA_UD_CID40\n",
      "Added new field: DA_HA_UD_CID43\n",
      "Added new field: DA_HP_LD_CID24\n",
      "Added new field: DA_HP_LD_CID25\n",
      "Added new field: DA_HP_LD_CID26\n",
      "Added new field: DA_HP_LD_CID27\n",
      "Added new field: DA_HP_LD_CID28\n",
      "Added new field: DA_HP_LD_CID29\n",
      "Added new field: DA_HP_LD_CID30\n",
      "Added new field: DA_HP_LD_CID31\n",
      "Added new field: DA_HP_LD_CID32\n",
      "Added new field: DA_HP_LG_CID42\n",
      "Added new field: DA_HP_LG_CID46\n",
      "Added new field: DA_HP_LG_CID50\n",
      "Added new field: DA_HP_LG_CID56\n",
      "Added new field: DA_HP_LG_CID57\n",
      "Added new field: DA_HP_UD_CID37\n",
      "Added new field: DA_HP_UD_CID38\n",
      "Added new field: DA_HP_UD_CID39\n",
      "Added new field: DA_HP_UD_CID40\n",
      "Added new field: DA_HP_UD_CID43\n",
      "Added new field: DA_HP_UD_CID45\n",
      "Added new field: DA_MA_LD_CID24\n",
      "Added new field: DA_MA_LD_CID25\n",
      "Added new field: DA_MA_LD_CID26\n",
      "Added new field: DA_MA_LD_CID27\n",
      "Added new field: DA_MA_LD_CID28\n",
      "Added new field: DA_MA_LD_CID29\n",
      "Added new field: DA_MA_LD_CID30\n",
      "Added new field: DA_MA_LD_CID31\n",
      "Added new field: DA_MA_LD_CID32\n",
      "Added new field: DA_MA_LG_CID42\n",
      "Added new field: DA_MA_LG_CID47\n",
      "Added new field: DA_MA_LG_CID48\n",
      "Added new field: DA_MA_LG_CID50\n",
      "Added new field: DA_MA_LG_CID54\n",
      "Added new field: DA_MA_UD_CID37\n",
      "Added new field: DA_MA_UD_CID38\n",
      "Added new field: DA_MA_UD_CID39\n",
      "Added new field: DA_MA_UD_CID40\n",
      "Added new field: DA_MA_UD_CID43\n",
      "Added new field: DA_MP_LD_CID24\n",
      "Added new field: DA_MP_LD_CID25\n",
      "Added new field: DA_MP_LD_CID26\n",
      "Added new field: DA_MP_LD_CID27\n",
      "Added new field: DA_MP_LD_CID28\n",
      "Added new field: DA_MP_LD_CID29\n",
      "Added new field: DA_MP_LD_CID30\n",
      "Added new field: DA_MP_LD_CID31\n",
      "Added new field: DA_MP_LD_CID32\n",
      "Added new field: DA_MP_LG_CID42\n",
      "Added new field: DA_MP_LG_CID50\n",
      "Added new field: DA_MP_LG_CID56\n",
      "Added new field: DA_MP_LG_CID57\n",
      "Added new field: DA_MP_UD_CID37\n",
      "Added new field: DA_MP_UD_CID38\n",
      "Added new field: DA_MP_UD_CID39\n",
      "Added new field: DA_MP_UD_CID40\n",
      "Added new field: DA_MP_UD_CID43\n",
      "Runtime: 6036.66 seconds\n",
      "Runtime: 100.61 minutes\n",
      "Runtime: 1.68 hours\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Calculate DA step 2 of 4: Determine DA for each component (if multiple available datasets for a single component, \n",
    "    DA is set to 1) \"\"\"\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL ###  \n",
    "\n",
    "### THIS CELL ONLY NEEDS EXECUTED ONCE ###\n",
    "\n",
    "### THIS CELL IS NEEDED IN THE FINAL SCRIPT ###\n",
    "\n",
    "\"\"\" NOTE: If this script is killed, it will result in an incomplete calculation of DA for a component field, resulting \n",
    "in empty cells for that field. This is an issue since the code is not setup to overwrite DA for components.  To \n",
    "resolve this, you need to delete the field (e.g., arcpy.DeleteField_management(in_table=PE_Grid, \n",
    "drop_field='DA_HA_UD_CID39')) and re-run this section of code. \n",
    "\n",
    "ADDENDUM: Added 'try' statement and necessary logic to address this automatically.  No action is needed by the user. \"\"\"\n",
    "\n",
    "######################################################################################################################\n",
    "def FieldValues(table, field):\n",
    "    \"\"\"\n",
    "    Create a list of unique values from a field in a feature class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table: <str>\n",
    "        Name of the table or feature class\n",
    "        \n",
    "    field: <str>\n",
    "        Name of the field \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    unique_values: <list>\n",
    "        Field values \n",
    "    \"\"\"\n",
    "    # Create a cursor object for reading the table\n",
    "    cursor = arcpy.da.SearchCursor(table, [field]) # A cursor iterates over rows in table\n",
    "\n",
    "    # Create an empty list for unique values\n",
    "    unique_values = []\n",
    "\n",
    "    # Iterate through rows\n",
    "    for row in cursor:\n",
    "        unique_values.append(row[0])\n",
    "    \n",
    "    return unique_values\n",
    "######################################################################################################################\n",
    "\n",
    "t_start = process_time()  # track processing time\n",
    "\n",
    "# Update field names\n",
    "field_names = ListFieldNames(PE_Grid)\n",
    "\n",
    "# Iterate through all components, create new field (if necessary), and determine DA\n",
    "for i in range(len(unique_components)):\n",
    "    \n",
    "#     LIMITER FOR DEVELOPMENT ONLY\n",
    "#     if i == 2:\n",
    "#         break\n",
    "        \n",
    "    # A list containing the unique component and corresponding feature datasets (that are represented as fields in PE_Grid)\n",
    "    component_fields =  [unique_components[i]] + components_data_array[i]\n",
    "\n",
    "    # Create a new field for unique_component if it does not already exist   \n",
    "    present = 0\n",
    "    for indiv_field in field_names:\n",
    "        if indiv_field == unique_components[i]:\n",
    "            present = present + 1\n",
    "        else:\n",
    "            present = present + 0\n",
    "    if not present:\n",
    "        # Add new field for unique_component\n",
    "        arcpy.AddField_management(PE_Grid, unique_components[i], \"SHORT\")\n",
    "        print(\"Added new field:\", unique_components[i])\n",
    "        # Assign DA value for each component\n",
    "        with arcpy.da.UpdateCursor(PE_Grid, component_fields) as cursor:\n",
    "            for row in cursor:\n",
    "                row[0] = 0\n",
    "                row[0] = max(row)\n",
    "                cursor.updateRow(row)  # Update the cursor with the updated list\n",
    "    else:\n",
    "        print(\"Field already exists for:\", unique_components[i])\n",
    "        try:\n",
    "            # The sum function will throw an error if there are any empty cells (due to this code being killed previously)\n",
    "            fv = FieldValues(PE_Grid, unique_components[i])\n",
    "            sum(fv)\n",
    "        except:\n",
    "            # If error, delete field and recalculate DA\n",
    "            print(\"Encountered error with:\", unique_components[i], \"\\n  ...trying again from scractch...\")\n",
    "            arcpy.DeleteField_management(in_table=PE_Grid, drop_field=unique_components[i])\n",
    "            arcpy.AddField_management(PE_Grid, unique_components[i], \"SHORT\")\n",
    "            print(\"Deleted and re-added field:\", unique_components[i])\n",
    "            with arcpy.da.UpdateCursor(PE_Grid, component_fields) as cursor:\n",
    "                for row in cursor:\n",
    "                    row[0] = 0\n",
    "                    row[0] = max(row)\n",
    "                    cursor.updateRow(row)  # Update the cursor with the updated list\n",
    "            fv = FieldValues(PE_Grid, unique_components[i])\n",
    "#             print(\"Sum:\", sum(fv))\n",
    "        \n",
    "# Update field names\n",
    "field_names = ListFieldNames(PE_Grid)\n",
    "\n",
    "\n",
    "# Print processing time\n",
    "t_stop = process_time()\n",
    "seconds = t_stop-t_start\n",
    "minutes = seconds/60\n",
    "hours = minutes/60\n",
    "\n",
    "print(\"Runtime:\", round(seconds, 2), \"seconds\")\n",
    "print(\"Runtime:\", round(minutes, 2), \"minutes\")\n",
    "print(\"Runtime:\", round(hours, 2), \"hours\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# QAQC - IN DEVELOPMENT\n",
    "\n",
    "check = FieldValues(PE_Grid, 'DA_Fl_LD_CID01_mrds_trim')\n",
    "check_df = pd.Series(check)\n",
    "check_df.describe()\n",
    "check_df.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### DEVELOPMENT ONLY ###\n",
    "\n",
    "# \"\"\" Determine which domain types have available datasets \"\"\"\n",
    "\n",
    "# domainTypes_ALL = [\"LD\", \"SD\", \"SA\", \"UD\"]\n",
    "# domainTypes = []\n",
    "\n",
    "# for domainType in domainTypes_ALL:\n",
    "#     count = [i for i in unique_components if domainType in i]\n",
    "#     if len(count) > 0:\n",
    "#         domainTypes.append(domainType)\n",
    "\n",
    "# print(domainTypes)\n",
    "        \n",
    "# \"\"\" Determine numer of available datasets for each domain type, if any \"\"\"\n",
    "\n",
    "# AllType_count = [i for i in unique_components if 'CID' in i]\n",
    "# LG_count = [i for i in unique_components if 'LG' in i]\n",
    "# LD_count = [i for i in unique_components if 'LD' in i]\n",
    "# SD_count = [i for i in unique_components if 'SD' in i]\n",
    "# UD_count = [i for i in unique_components if 'UD' in i]\n",
    "\n",
    "# count = LD_count + SD_count + UD_count + LG_count \n",
    "\n",
    "# print(\"Number of components with datasets:\", len(AllType_count), \"\\n\")\n",
    "# print(\"LG:\", len(LG_count), \"\\n\")\n",
    "# print(\"LD:\", len(LD_count))\n",
    "# print(\"SD:\", len(SD_count))\n",
    "# print(\"UD:\", len(UD_count), \"\\n\")\n",
    "\n",
    "# # print(\"\\nSum:\", len(count), \"\\n\")\n",
    "\n",
    "# # Verify which indicies are in PE_Grid\n",
    "# field_names = ListFieldNames(PE_Grid)\n",
    "# index_yn = [i for i in field_names if 'index' in i]\n",
    "# print(\"Indicies in PE_Grid:\", index_yn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding LG_index components to master DataFrame...\n",
      "\n",
      "LD distribution started...\n",
      "created list of domain index values\n",
      "created dataframe with values for records\n",
      "LD distribution finished.\n",
      "\n",
      "UD distribution started...\n",
      "created list of domain index values\n",
      "created dataframe with values for records\n",
      "UD distribution finished.\n",
      "\n",
      "All domain types distributed.\n",
      "\n",
      "Runtime: 82.7 seconds\n",
      "Runtime: 1.38 minutes\n",
      "Runtime: 0.02 hours\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Calculate DA step 3 of 4: Distribute DA across appropriate domain areas.  Assigns presence/absesnce \n",
    "    for a dataset within a geologic domain.  Also creates a dictionary of DataFrames ('df_dict_LG_domains_ALL') for \n",
    "    each component spatial type (e.g., 'LD') post-spatial distribution, and a master DataFrame with all components \n",
    "    (local and domains) \"\"\"\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL ###\n",
    "\n",
    "### THIS CELL NEEDS EXECUTED EACH SESSION ###\n",
    "\n",
    "### THIS CELL WILL BE NEEDED IN THE FINAL SCRIPT ###\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "def FieldValues(table, field):\n",
    "    \"\"\"\n",
    "    Create a list of unique values from a field in a feature class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table: <str>\n",
    "        Name of the table or feature class\n",
    "        \n",
    "    field: <str>\n",
    "        Name of the field \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    unique_values: <list>\n",
    "        Field values \n",
    "    \"\"\"\n",
    "    # Create a cursor object for reading the table\n",
    "    cursor = arcpy.da.SearchCursor(table, [field]) # A cursor iterates over rows in table\n",
    "\n",
    "    # Create an empty list for unique values\n",
    "    unique_values = []\n",
    "\n",
    "    # Iterate through rows\n",
    "    for row in cursor:\n",
    "        unique_values.append(row[0])\n",
    "    \n",
    "    return unique_values\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "t_start = process_time()  # track processing time\n",
    "\n",
    "# Create a list of local grid index values, then create DataFrame\n",
    "LG_index_values = FieldValues(PE_Grid,'LG_index')\n",
    "df_index_cols = pd.DataFrame(LG_index_values, columns = {'LG_index'})\n",
    "df_index_cols.set_index('LG_index', inplace=True)  # Set 'LG_index' as dataframe index\n",
    "df_dict_LG_domains_ALL = {\"indicies\": df_index_cols}  # This dict will contain all of the calculated DA fields\n",
    "\n",
    "# Add LG components to master DataFrame \"df_dict_LG_domains_ALL\"  \n",
    "print(\"Adding LG_index components to master DataFrame...\\n\")\n",
    "LG_components = [i for i in unique_components if 'LG' in i]  # List of LG components\n",
    "LG_cols = {'LG_index': LG_index_values}  # Include LG_index for joining\n",
    "for i in LG_components:\n",
    "    LG_cols[i] = FieldValues(PE_Grid, i)\n",
    "df_LG_fieldvalues = pd.DataFrame(LG_cols)\n",
    "df_LG_fieldvalues.set_index('LG_index', inplace=True)  # Set 'LG_index' as dataframe index\n",
    "df_dict_LG_domains_ALL['LG'] = df_index_cols.join(df_LG_fieldvalues)\n",
    "\n",
    "# Determine which domain types have available datasets for the AOI\n",
    "domainTypes_ALL = [\"LD\", \"SD\", \"SA\", \"UD\"]\n",
    "domainTypes = []\n",
    "for domainType in domainTypes_ALL:\n",
    "    count = [i for i in unique_components if domainType in i]\n",
    "    if len(count) > 0:\n",
    "        domainTypes.append(domainType)\n",
    "\n",
    "# Create a dict of lists for each domain type (components with domain subtext (e.g., LD) in the name)\n",
    "domainType_components = {}\n",
    "        \n",
    "# Distribute presence across domain for each domain type\n",
    "for domainType in domainTypes:\n",
    "\n",
    "    print(domainType, \"distribution started...\")\n",
    "    \n",
    "    # Create a list of domain index values (e.g, LD1, LD2, LD3, LD4), then add to DataFrame\n",
    "    domainType_index_values = FieldValues(PE_Grid, domainType + '_index')\n",
    "    df_index_cols[domainType + '_index'] = domainType_index_values\n",
    "    df_index_cols.fillna(value={domainType + '_index': 0}, inplace=True)\n",
    "    print(\"created list of domain index values\")\n",
    "    \n",
    "    # Update dict of lists for each domain type\n",
    "    domainType_components[domainType] = [i for i in unique_components if domainType in i]\n",
    "\n",
    "    # Create DataFrame with values for records for each domainType\n",
    "    domain_cols = {'LG_index': LG_index_values}  # Include LG_index for joining\n",
    "    for i in domainType_components[domainType]:\n",
    "        domain_cols[i] = FieldValues(PE_Grid, i)\n",
    "    df_domainType_fieldvalues = pd.DataFrame(domain_cols)\n",
    "    df_domainType_fieldvalues.set_index('LG_index', inplace=True)  # Set 'LG_index' as dataframe index\n",
    "    print(\"created dataframe with values for records\")\n",
    "\n",
    "    # Join into a new DataFrame the domainType_index and domainType_components/values columns\n",
    "    df_domainType_joined = df_index_cols.join(df_domainType_fieldvalues, sort=False)\n",
    "\n",
    "    # Group by unique domainType_index values\n",
    "    df_domainType_grouped = df_domainType_joined.groupby([domainType + '_index'])\n",
    "\n",
    "    # Determine max of DA for each domainType_index group, return in \"DA_...domainType..._distributed\" column\n",
    "    df_domainType_max = df_domainType_grouped.max()\n",
    "    for i in domainType_components[domainType]:\n",
    "        df_domainType_max.rename(columns={i:(i + \"_distributed\")}, inplace=True)\n",
    "#     df_domainType_max.drop(['LG_index'], axis=1, inplace=True)  # LG_index is erroneously overwritten without this line\n",
    "\n",
    "    # Join index and DA_max columns in a new DataFrame\n",
    "#     df_domainType_export = df_index_cols.merge(df_domainType_max, on = domainType + '_index')\n",
    "    df_domainType_export = df_index_cols.join(df_domainType_max, on = domainType + '_index')\n",
    "#     df_domainType_all = df_domainType_joined.merge(df_domainType_max, on = domainType + '_index')\n",
    "\n",
    "    # Combine all domain types into a list/dict of DataFrames\n",
    "#     df_domainALL = df_domainType_joined.join(df_domainType_export, on='LG_index', lsuffix='', rsuffix='_from'+domainType)\n",
    "    df_dict_LG_domains_ALL[domainType] = df_domainType_export.copy()\n",
    "    \n",
    "    print(domainType, \"distribution finished.\\n\")\n",
    "    \n",
    "print(\"All domain types distributed.\\n\")\n",
    "  \n",
    "    \n",
    "# Compile a master dataframe in 'df_dict_LG_domains_ALL' for all spatial types (local and domains)\n",
    "spatialTypes = domainTypes.copy()  # Create a list for all spatial types\n",
    "spatialTypes.insert(0, 'LG')  # Include 'LG_index' for joining\n",
    "df_dict_LG_domains_ALL['compiled'] = df_dict_LG_domains_ALL['indicies'].copy()  # This is the master dataframe\n",
    "for i in spatialTypes:\n",
    "    df_dict_LG_domains_ALL['compiled'] = df_dict_LG_domains_ALL['compiled'].join(df_dict_LG_domains_ALL[i], lsuffix='', rsuffix='_from'+i)\n",
    "\n",
    "# Create a copy of LG component columns (named \"..._local\") to serve as QA/QC record\n",
    "for i in LG_components:\n",
    "    df_dict_LG_domains_ALL['LG'][i + \"_local\"] = df_dict_LG_domains_ALL['LG'][i].copy()\n",
    "\n",
    "\n",
    "### SECTION BELOW MAY BE COMMENTED OUT IF FILES ALREADY EXIST ###\n",
    "\n",
    "# # Export dataframes to csv and ArcGIS tables\n",
    "# for domainType in domainTypes:\n",
    "\n",
    "#     print(domainType, \"export started...\")\n",
    "    \n",
    "#     # Export domainType DataFrame as CSV\n",
    "#     exported_df_domainType = workspace_dir + \"/\" + domainType + r\"_domains_exported_df.csv\"\n",
    "#     df_dict_LG_domains_ALL[domainType].to_csv(exported_df_domainType, index=False)\n",
    "\n",
    "#     # Convert the DataFrame CSV file to ArcGIS table\n",
    "#     inTable = exported_df_domainType\n",
    "#     outLocation = workspace\n",
    "#     outTable = str(domainType + \"_domain_distributed_df_table\")\n",
    "#     try:\n",
    "#         arcpy.TableToTable_conversion(inTable, outLocation, outTable)\n",
    "#     except:\n",
    "#         print(str(domainType + \"_index\"), \"ArcGIS table already exists... deleting and trying again!\")\n",
    "#         arcpy.Delete_management(outTable)\n",
    "#         arcpy.TableToTable_conversion(inTable, outLocation, outTable)\n",
    "#         print(str(domainType + \"_index\"), \"DataFrame csv converted to ArcGIS table!\")\n",
    "    \n",
    "#     # Join DataFrame table to PE_Grid\n",
    "#     inFeatures = PE_Grid\n",
    "#     joinField = \"LG_index\"\n",
    "#     joinTable = outTable\n",
    "#     fieldList = list(df_dict_LG_domains_ALL[domainType].columns) \n",
    "#     fieldList.remove(\"LG_index\")  # exclude indicies from fields to join \n",
    "#     for domType in domainTypes:\n",
    "#         try:\n",
    "#             fieldList.remove(str(domType + \"_index\")) # exclude indicies from fields to join\n",
    "#         except:\n",
    "#             print(\"Unable to remove unnecessary fields from Join list... they may not exist.\")\n",
    "#     print(\"Joining\", joinTable, \"to\", PE_Grid)\n",
    "#     arcpy.JoinField_management(inFeatures, joinField, joinTable, joinField, fieldList)\n",
    "    \n",
    "#     print(domainType, \"exported.\\n\")\n",
    "\n",
    "# print('\\nAll done.')\n",
    "\n",
    "# # number of cells with data in each LD domain\n",
    "# df_domainALL['LD']['LD_index'].value_counts()\n",
    "\n",
    "# # number of cells with data in each UD domain\n",
    "# df_domainALL['UD']['UD_index'].value_counts()\n",
    "\n",
    "\n",
    "# Print processing time\n",
    "t_stop = process_time()\n",
    "seconds = t_stop-t_start\n",
    "minutes = seconds/60\n",
    "hours = minutes/60\n",
    "\n",
    "print(\"Runtime:\", round(seconds, 2), \"seconds\")\n",
    "print(\"Runtime:\", round(minutes, 2), \"minutes\")\n",
    "print(\"Runtime:\", round(hours, 2), \"hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 165.42 seconds\n",
      "Runtime: 2.76 minutes\n",
      "Runtime: 0.05 hours\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Calculate DA step 4 of 4: Calculate sum(DA) for each REE emplacement type (explicit tally of components;\n",
    "    not implicit score) \"\"\"\n",
    "\n",
    "### THIS CODE WILL BE IN FINAL SCRIPT ###\n",
    "\n",
    "### TESTED AND SUCCESSFUL ###\n",
    "\n",
    "t_start = process_time()  # track processing time\n",
    "\n",
    "# Comprehensive list of all possible components, including those deemed 'not testable' and \n",
    "# 'not evalutated (duplicate)'.  This list current as of 2020-03-24.  Values copied from Google Sheet\n",
    "# \"REE Enrichment Tree Related Data - Google Sheets 'Component_Codes_asof_2020-03-24'!Y2:FR2\"\n",
    "componentsALL = ['DA_Fl_LD_CID01', 'DA_Fl_LD_CID02', 'DA_Fl_LD_CID03', 'DA_Fl_LD_CID04', 'DA_Fl_LD_CID05', \n",
    "                 'DA_Fl_LD_CID06', 'DA_Fl_LD_CID07', 'DA_Fl_LD_CID08', 'DA_Fl_LD_CID09', 'DA_Eo_LD_CID10', \n",
    "                 'DA_Fl_NE_CID11', 'DA_Fl_NE_CID12', 'DA_Fl_NE_CID13', 'DA_Eo_LG_CID14', 'DA_Eo_LG_CID15', \n",
    "                 'DA_Eo_LD_CID16', 'DA_Fl_LD_CID17', 'DA_Fl_LG_CID18', 'DA_Fl_NT_CID19', 'DA_Fl_LD_CID19', \n",
    "                 'DA_Eo_NT_CID20', 'DA_Fl_NT_CID20', 'DA_Eo_NT_CID21', 'DA_Eo_LD_CID21', 'DA_Eo_NE_CID21', \n",
    "                 'DA_Eo_NT_CID22', 'DA_Eo_NT_CID23', 'DA_MA_LD_CID24', 'DA_MA_LD_CID25', 'DA_MA_LD_CID26', \n",
    "                 'DA_MA_LD_CID27', 'DA_MA_LD_CID28', 'DA_MA_LD_CID29', 'DA_MA_LD_CID30', 'DA_MA_LD_CID31', \n",
    "                 'DA_MA_LD_CID32', 'DA_MA_NE_CID33', 'DA_MA_NE_CID34', 'DA_MA_NE_CID35', 'DA_MA_NE_CID36', \n",
    "                 'DA_MA_UD_CID37', 'DA_MA_UD_CID38', 'DA_MA_UD_CID39', 'DA_MA_UD_CID40', 'DA_MA_UD_CID41', \n",
    "                 'DA_MA_LG_CID42', 'DA_MA_UD_CID43', 'DA_MA_NT_CID44', 'DA_HP_UD_CID45', 'DA_HP_LG_CID46', \n",
    "                 'DA_MA_LG_CID47', 'DA_MA_LG_CID48', 'DA_MA_LG_CID49', 'DA_MA_LG_CID50', 'DA_MA_NT_CID51', \n",
    "                 'DA_MA_LG_CID52', 'DA_MA_NT_CID53', 'DA_MA_LG_CID54', 'DA_MP_NT_CID55', 'DA_MP_LG_CID56', \n",
    "                 'DA_MP_LG_CID57', 'DA_MP_NT_CID58', 'DA_MA_NT_CID59', 'DA_Fl_LD_CID10', 'DA_Fl_NT_CID21', \n",
    "                 'DA_Fl_LD_CID21', 'DA_Fl_NE_CID21', 'DA_Fl_NT_CID22', 'DA_Fl_NT_CID23', 'DA_MP_LD_CID24', \n",
    "                 'DA_MP_LD_CID25', 'DA_MP_LD_CID26', 'DA_MP_LD_CID27', 'DA_MP_LD_CID28', 'DA_MP_LD_CID29', \n",
    "                 'DA_MP_LD_CID30', 'DA_MP_LD_CID31', 'DA_MP_LD_CID32', 'DA_MP_NE_CID33', 'DA_MP_NE_CID34', \n",
    "                 'DA_MP_NE_CID35', 'DA_MP_NE_CID36', 'DA_MP_UD_CID37', 'DA_MP_UD_CID38', 'DA_MP_UD_CID39', \n",
    "                 'DA_MP_UD_CID40', 'DA_MP_UD_CID41', 'DA_MP_LG_CID42', 'DA_MP_UD_CID43', 'DA_MP_NT_CID44', \n",
    "                 'DA_HA_LG_CID47', 'DA_HA_LG_CID48', 'DA_HA_LG_CID49', 'DA_MP_LG_CID50', 'DA_MP_NT_CID51', \n",
    "                 'DA_MP_LG_CID52', 'DA_MP_NT_CID53', 'DA_HA_LG_CID54', 'DA_HP_NT_CID55', 'DA_HP_LG_CID56', \n",
    "                 'DA_HP_LG_CID57', 'DA_HP_NT_CID58', 'DA_HA_NT_CID59', 'DA_HA_LD_CID24', 'DA_HA_LD_CID25', \n",
    "                 'DA_HA_LD_CID26', 'DA_HA_LD_CID27', 'DA_HA_LD_CID28', 'DA_HA_LD_CID29', 'DA_HA_LD_CID30', \n",
    "                 'DA_HA_LD_CID31', 'DA_HA_LD_CID32', 'DA_HA_NE_CID33', 'DA_HA_NE_CID34', 'DA_HA_NE_CID35', \n",
    "                 'DA_HA_NE_CID36', 'DA_HA_UD_CID37', 'DA_HA_UD_CID38', 'DA_HA_UD_CID39', 'DA_HA_UD_CID40', \n",
    "                 'DA_HA_UD_CID41', 'DA_HA_LG_CID42', 'DA_HA_UD_CID43', 'DA_HA_UD_CID45', 'DA_HA_LG_CID50', \n",
    "                 'DA_HA_NT_CID51', 'DA_HA_LG_CID52', 'DA_HA_NT_CID53', 'DA_HP_LD_CID24', 'DA_HP_LD_CID25', \n",
    "                 'DA_HP_LD_CID26', 'DA_HP_LD_CID27', 'DA_HP_LD_CID28', 'DA_HP_LD_CID29', 'DA_HP_LD_CID30', \n",
    "                 'DA_HP_LD_CID31', 'DA_HP_LD_CID32', 'DA_HP_NE_CID33', 'DA_HP_NE_CID34', 'DA_HP_NE_CID35', \n",
    "                 'DA_HP_NE_CID36', 'DA_HP_UD_CID37', 'DA_HP_UD_CID38', 'DA_HP_UD_CID39', 'DA_HP_UD_CID40', \n",
    "                 'DA_HP_UD_CID41', 'DA_HP_LG_CID42', 'DA_HP_UD_CID43', 'DA_HP_LG_CID50', 'DA_HP_NT_CID51']\n",
    "\n",
    "\n",
    "# Create dict of unique_components for each emplacement mechanism with data in the geodatabase\n",
    "mechanismTypes = ['Eo', 'Fl', 'HA', 'HP', 'MA', 'MP']\n",
    "mechanismComponents = {}\n",
    "for mechanism in mechanismTypes:\n",
    "    mechanismComponents[mechanism]= [i for i in unique_components if mechanism in i]\n",
    "\n",
    "# Create dict of all possible components (including those that are untestable) for each emplacement mechanism\n",
    "mechanismComponentsALL = {}\n",
    "for mechanism in mechanismTypes:\n",
    "    mechanismComponentsALL[mechanism]= sorted([i for i in componentsALL if mechanism in i])\n",
    "\n",
    "# # DEVELOPMENT ONLY: Create a list of components that are 'not testable' or 'not evaluated', set to 'False'\n",
    "# NotTestable = []\n",
    "# for k in mechanismComponentsALL.keys():\n",
    "#     I = [x for x in mechanismComponentsALL[k] if 'N' in x]\n",
    "#     for i in I: \n",
    "#         NotTestable.append(i)\n",
    "# for i in NotTestable:\n",
    "#     df_PE_calc[i] = False\n",
    "\n",
    "# Create a copy of the DataFrame that contains post-processed tallies.  \n",
    "df_keys = df_dict_LG_domains_ALL.keys()\n",
    "df_dict_PostProcessed = {}\n",
    "for key in df_keys:\n",
    "    df_dict_PostProcessed[key] = df_dict_LG_domains_ALL[key].copy()\n",
    "\n",
    "# Rename fields to simplified component names (e.g., remove \"_distributed\", etc.)\n",
    "sel = df_dict_PostProcessed['compiled'].columns  # list of fields\n",
    "rename_fields = [i for i in sel if \"_distributed\" in i]  # list of fields to be renamed\n",
    "for i in rename_fields:\n",
    "    df_dict_PostProcessed['compiled'].rename(columns={i:(i[:14])}, inplace=True)\n",
    "\n",
    "    # Create an 'empty' dataframe that has columns for all components and row values = 'False'\n",
    "df_PE_empty = df_dict_LG_domains_ALL['indicies'].copy()\n",
    "for component in componentsALL:\n",
    "    df_PE_empty[component] = False\n",
    "\n",
    "# Create new DataFrame (df_PE_calc), update values from PostProcessed DataFrame\n",
    "df_PE_calc = df_PE_empty.copy()\n",
    "df_PE_calc.update(df_dict_PostProcessed['compiled'])\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "### Generic assignment for all cells (DA)\n",
    "df_PE_calc['DA_Eo_NT_CID20'] = True  # Accumulation of peat\n",
    "df_PE_calc['DA_Fl_NT_CID20'] = True  # Accumulation of peat\n",
    "\n",
    "df_PE_calc['DA_Fl_NT_CID22'] = True  # Burial of peat\n",
    "\n",
    "df_PE_calc['DA_Fl_NT_CID23'] = True  # Conversion of peat to coal\n",
    "\n",
    "df_PE_calc['DA_HA_LG_CID52'] = True # Coal and/or related strata\n",
    "df_PE_calc['DA_HP_LG_CID52'] = True # Coal and/or related strata\n",
    "df_PE_calc['DA_MA_LG_CID52'] = True # Coal and/or related strata\n",
    "df_PE_calc['DA_MP_LG_CID52'] = True # Coal and/or related strata\n",
    "\n",
    "### Powder River Basin assignment for all cells (DA)\n",
    "df_PE_calc['DA_Eo_LG_CID14'] = True  # Mire downwind of volcanism (this is true for PRB)\n",
    "df_PE_calc['DA_Fl_LD_CID17'] = True  # Mire in same paleo-drainage basin\n",
    "df_PE_calc['DA_Fl_LG_CID18'] = True  # Mire downstream of REE source\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "### Eo relevant components.  Not testable: CID15, CID21\n",
    "\n",
    "\n",
    "### Fl relevant components.  Not testable: CID17, CID18, CID19, CID21\n",
    "df_PE_calc['DA_Fl_NE_CID11'] = df_PE_calc[['DA_Fl_LD_CID01', 'DA_Fl_LD_CID02', 'DA_Fl_LD_CID03', 'DA_Fl_LD_CID04', \n",
    "                                           'DA_Fl_LD_CID05', 'DA_Fl_LD_CID06']].max(axis=1)  # Bedrock REE deposit\n",
    "df_PE_calc['DA_Fl_NE_CID12'] = df_PE_calc[['DA_Fl_LD_CID07', 'DA_Fl_LD_CID08', 'DA_Fl_LD_CID09']].max(axis=1)  # Sed REE deposit\n",
    "df_PE_calc['DA_Fl_NE_CID13'] = df_PE_calc[['DA_Fl_LD_CID10', 'DA_Fl_NE_CID11', 'DA_Fl_NE_CID12']].max(axis=1)  # REE source\n",
    "\n",
    "\n",
    "### HA relevant components.  Not testable: CID47, CID48, CID49, CID51, CID53, CID59\n",
    "df_PE_calc['DA_HA_NE_CID33'] = df_PE_calc[['DA_HA_UD_CID37', 'DA_HA_UD_CID38', 'DA_HA_UD_CID39', \n",
    "                                           'DA_HA_UD_CID40', 'DA_HA_UD_CID41']].max(axis=1)  # Alkaline volcanic ash\n",
    "df_PE_calc['DA_HA_NE_CID34'] = df_PE_calc[['DA_HA_LD_CID24', 'DA_HA_LD_CID25', 'DA_HA_LD_CID26',\n",
    "                                           'DA_HA_LD_CID27', 'DA_HA_LD_CID28', 'DA_HA_LD_CID29']].max(axis=1)  # Bedrock REE deposit\n",
    "df_PE_calc['DA_HA_NE_CID35'] = df_PE_calc[['DA_HA_LD_CID30', 'DA_HA_LD_CID31', 'DA_HA_LD_CID32']].max(axis=1)  # Sed REE deposit\n",
    "df_PE_calc['DA_HA_NE_CID36'] = df_PE_calc[['DA_HA_NE_CID33', 'DA_HA_NE_CID34', 'DA_HA_NE_CID35']].max(axis=1)  # REE source\n",
    "df_PE_calc['DA_HA_NE_42_43'] = df_PE_calc[['DA_HA_LG_CID42', 'DA_HA_UD_CID43']].max(axis=1)  # Conduit for fluid flow\n",
    "\n",
    "\n",
    "### HP relevant components.  Not testable: CID47, CID48, CID49, CID51, CID53, CID55, CID58\n",
    "df_PE_calc['DA_HP_NE_CID33'] = df_PE_calc[['DA_HP_UD_CID37', 'DA_HP_UD_CID38', 'DA_HP_UD_CID39', \n",
    "                                           'DA_HP_UD_CID40', 'DA_HP_UD_CID41']].max(axis=1)  # Alkaline volcanic ash\n",
    "df_PE_calc['DA_HP_NE_CID34'] = df_PE_calc[['DA_HP_LD_CID24', 'DA_HP_LD_CID25', 'DA_HP_LD_CID26',\n",
    "                                           'DA_HP_LD_CID27', 'DA_HP_LD_CID28', 'DA_HP_LD_CID29']].max(axis=1)  # Bedrock REE deposit\n",
    "df_PE_calc['DA_HP_NE_CID35'] = df_PE_calc[['DA_HP_LD_CID30', 'DA_HP_LD_CID31', 'DA_HP_LD_CID32']].max(axis=1)  # Sed REE deposit\n",
    "df_PE_calc['DA_HP_NE_CID36'] = df_PE_calc[['DA_HP_NE_CID33', 'DA_HP_NE_CID34', 'DA_HP_NE_CID35']].max(axis=1)  # REE source\n",
    "df_PE_calc['DA_HP_NE_42_43'] = df_PE_calc[['DA_HP_LG_CID42', 'DA_HP_UD_CID43']].max(axis=1)  # Conduit for fluid flow\n",
    "df_PE_calc['DA_HP_NE_57_46'] = df_PE_calc[['DA_HP_LG_CID57', 'DA_HP_LG_CID46']].max(axis=1)  # Dissolve phosphorus\n",
    "\n",
    "\n",
    "### MA relevant components.  Not testable:  CID44, CID47, CID48, CID49, CID51, CID53, CID59\n",
    "df_PE_calc['DA_MA_NE_CID33'] = df_PE_calc[['DA_MA_UD_CID37', 'DA_MA_UD_CID38', 'DA_MA_UD_CID39', \n",
    "                                           'DA_MA_UD_CID40', 'DA_MA_UD_CID41']].max(axis=1)  # Alkaline volcanic ash\n",
    "df_PE_calc['DA_MA_NE_CID34'] = df_PE_calc[['DA_MA_LD_CID24', 'DA_MA_LD_CID25', 'DA_MA_LD_CID26',\n",
    "                                           'DA_MA_LD_CID27', 'DA_MA_LD_CID28', 'DA_MA_LD_CID29']].max(axis=1)  # Bedrock REE deposit\n",
    "df_PE_calc['DA_MA_NE_CID35'] = df_PE_calc[['DA_MA_LD_CID30', 'DA_MA_LD_CID31', 'DA_MA_LD_CID32']].max(axis=1)  # Sed REE deposit\n",
    "df_PE_calc['DA_MA_NE_CID36'] = df_PE_calc[['DA_MA_NE_CID33', 'DA_MA_NE_CID34', 'DA_MA_NE_CID35']].max(axis=1)  # REE source\n",
    "df_PE_calc['DA_MA_NE_42_43'] = df_PE_calc[['DA_MA_LG_CID42', 'DA_MA_UD_CID43']].max(axis=1)  # Conduit for fluid flow\n",
    "\n",
    "\n",
    "### MP relevant components.  Not testable: CID47, CID48, CID49, CID51, CID53, CID55, CID58\n",
    "df_PE_calc['DA_MP_NE_CID33'] = df_PE_calc[['DA_MP_UD_CID37', 'DA_MP_UD_CID38', 'DA_MP_UD_CID39', \n",
    "                                           'DA_MP_UD_CID40', 'DA_MP_UD_CID41']].max(axis=1)  # Alkaline volcanic ash\n",
    "df_PE_calc['DA_MP_NE_CID34'] = df_PE_calc[['DA_MP_LD_CID24', 'DA_MP_LD_CID25', 'DA_MP_LD_CID26',\n",
    "                                           'DA_MP_LD_CID27', 'DA_MP_LD_CID28', 'DA_MP_LD_CID29']].max(axis=1)  # Bedrock REE deposit\n",
    "df_PE_calc['DA_MP_NE_CID35'] = df_PE_calc[['DA_MP_LD_CID30', 'DA_MP_LD_CID31', 'DA_MP_LD_CID32']].max(axis=1)  # Sed REE deposit\n",
    "df_PE_calc['DA_MP_NE_CID36'] = df_PE_calc[['DA_MP_NE_CID33', 'DA_MP_NE_CID34', 'DA_MP_NE_CID35']].max(axis=1)  # REE source\n",
    "df_PE_calc['DA_MP_NE_42_43'] = df_PE_calc[['DA_MP_LG_CID42', 'DA_MP_UD_CID43']].max(axis=1)  # Conduit for fluid flow\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "# DR components (NOTE: this is NOT the entire list of DR components; only those that are considered testable)\n",
    "DR_Eo = ['DA_Eo_LD_CID10', 'DA_Eo_LG_CID14', 'DA_Eo_LD_CID16', 'DA_Fl_NT_CID22', 'DA_Fl_NT_CID23']\n",
    "DR_Fl = ['DA_Fl_NE_CID13', 'DA_Fl_NT_CID20', 'DA_Fl_NT_CID22', 'DA_Fl_NT_CID23']\n",
    "DR_HA = ['DA_HA_NE_42_43', 'DA_HA_LG_CID52', 'DA_HA_NE_CID36', 'DA_HA_UD_CID45', 'DA_HA_LG_CID50', 'DA_HA_LG_CID54']\n",
    "DR_HP = ['DA_HP_NE_42_43', 'DA_HP_LG_CID52', 'DA_HP_NE_CID36', 'DA_HP_UD_CID45', 'DA_HP_LG_CID50', 'DA_HP_LG_CID56', 'DA_HP_NE_57_46']\n",
    "DR_MA = ['DA_MA_NE_42_43', 'DA_MA_LG_CID52', 'DA_MA_NE_CID36', 'DA_MA_LG_CID50', 'DA_MA_LG_CID54']\n",
    "DR_MP = ['DA_MP_NE_42_43', 'DA_MP_LG_CID52', 'DA_MP_NE_CID36', 'DA_MP_LG_CID50', 'DA_MP_LG_CID56']\n",
    "\n",
    "DR_Types = [DR_Eo, DR_Fl, DR_HA, DR_HP, DR_MA, DR_MP]  # A list of required components (DR) for each mechanism type\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "# Add sum fields to dataframe\n",
    "df_PE_calc['Eo_sum'] = df_PE_calc[DR_Eo].sum(axis=1)\n",
    "df_PE_calc['Fl_sum'] = df_PE_calc[DR_Fl].sum(axis=1)\n",
    "df_PE_calc['HA_sum'] = df_PE_calc[DR_HA].sum(axis=1)\n",
    "df_PE_calc['HP_sum'] = df_PE_calc[DR_HP].sum(axis=1)\n",
    "df_PE_calc['MA_sum'] = df_PE_calc[DR_MA].sum(axis=1)\n",
    "df_PE_calc['MP_sum'] = df_PE_calc[DR_MP].sum(axis=1)\n",
    "\n",
    "# # Display DA_sums\n",
    "# DAsum_cols = []\n",
    "# for mech in mechanismTypes:\n",
    "#     DAsum_cols.append(mech + '_sum')\n",
    "# df_PE_calc[DAsum_cols].describe()\n",
    "    \n",
    "# Calculate DA_sum/DR\n",
    "DAsumDR_cols = []  # To be columns of DA_sum / DR\n",
    "for i in range(len(DR_Types)):\n",
    "    col = FeatureDataset + '_' + DR_Types[i][0][3:5] + '_sum_DR'  # Assemble column heading (e.g., 'DA_Eo_sum_DR')\n",
    "    df_PE_calc[col] = df_PE_calc[DR_Types[i][0][3:5] + '_sum'] / len(DR_Types[i])  # Divide mechanism sum by DR (e.g., Eo_sum / DR_Eo)\n",
    "    DAsumDR_cols.append(col)  # Append column name to this list\n",
    "\n",
    "# df_PE_calc[DAsumDR_cols].describe()\n",
    "\n",
    "\n",
    "# Print processing time\n",
    "t_stop = process_time()\n",
    "seconds = t_stop-t_start\n",
    "minutes = seconds/60\n",
    "hours = minutes/60\n",
    "\n",
    "print(\"Runtime:\", round(seconds, 2), \"seconds\")\n",
    "print(\"Runtime:\", round(minutes, 2), \"minutes\")\n",
    "print(\"Runtime:\", round(hours, 2), \"hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DA_Eo_sum_DR</th>\n",
       "      <th>DA_Fl_sum_DR</th>\n",
       "      <th>DA_HA_sum_DR</th>\n",
       "      <th>DA_HP_sum_DR</th>\n",
       "      <th>DA_MA_sum_DR</th>\n",
       "      <th>DA_MP_sum_DR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>188750.0</td>\n",
       "      <td>188750.0</td>\n",
       "      <td>188750.000000</td>\n",
       "      <td>188750.000000</td>\n",
       "      <td>188750.000000</td>\n",
       "      <td>188750.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.531179</td>\n",
       "      <td>0.612026</td>\n",
       "      <td>0.637415</td>\n",
       "      <td>0.637415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095192</td>\n",
       "      <td>0.121746</td>\n",
       "      <td>0.114230</td>\n",
       "      <td>0.114230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DA_Eo_sum_DR  DA_Fl_sum_DR   DA_HA_sum_DR   DA_HP_sum_DR  \\\n",
       "count      188750.0      188750.0  188750.000000  188750.000000   \n",
       "mean            1.0           1.0       0.531179       0.612026   \n",
       "std             0.0           0.0       0.095192       0.121746   \n",
       "min             1.0           1.0       0.333333       0.428571   \n",
       "25%             1.0           1.0       0.500000       0.571429   \n",
       "50%             1.0           1.0       0.500000       0.571429   \n",
       "75%             1.0           1.0       0.500000       0.571429   \n",
       "max             1.0           1.0       0.833333       1.000000   \n",
       "\n",
       "        DA_MA_sum_DR   DA_MP_sum_DR  \n",
       "count  188750.000000  188750.000000  \n",
       "mean        0.637415       0.637415  \n",
       "std         0.114230       0.114230  \n",
       "min         0.400000       0.400000  \n",
       "25%         0.600000       0.600000  \n",
       "50%         0.600000       0.600000  \n",
       "75%         0.600000       0.600000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate DA_sum/DR\n",
    "DR_Types = [DR_Eo, DR_Fl, DR_HA, DR_HP, DR_MA, DR_MP]  # A list of required components (DR) for each mechanism type\n",
    "DAsumDR_cols = []  # To be columns of DA_sum / DR\n",
    "for i in range(len(DR_Types)):\n",
    "    col = FeatureDataset + '_' + DR_Types[i][0][3:5] + '_sum_DR'  # Assemble column heading (e.g., 'DA_Eo_sum_DR')\n",
    "    df_PE_calc[col] = df_PE_calc[DR_Types[i][0][3:5] + '_sum'] / len(DR_Types[i])  # Divide mechanism sum by DR (e.g., Eo_sum / DR_Eo)\n",
    "    DAsumDR_cols.append(col)  # Append column name to this list\n",
    "    \n",
    "df_PE_calc[DAsumDR_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEVELOPMENT ONLY ###\n",
    "\n",
    "# Export DataFrame as CSV\n",
    "# exported_df = workspace_dir + \"/\" + r\"exported_df.csv\"\n",
    "# df_PE_calc.to_csv(exported_df)\n",
    "\n",
    "# exported_df_minCols = workspace_dir + \"/\" + r\"exported_df_minCols.csv\"\n",
    "# df_PE_calc.to_csv(exported_df_minCols, columns=DAsumDR_cols)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### THIS CODE IS REPLACED BY A FUNCTION; NO LONGER NEEDED ###\n",
    "\n",
    "### TESTED AND SUCCESSFUL ###\n",
    "\n",
    "# Export dataframes to csv and ArcGIS tables\n",
    "\n",
    "print(\"DataFrame export started...\")\n",
    "\n",
    "# # Export DataFrame as CSV\n",
    "# exported_df = workspace_dir + \"/\" + r\"exported_df.csv\"\n",
    "# df_PE_calc.to_csv(exported_df)\n",
    "# print(\"DataFrame exported as csv.\")\n",
    "\n",
    "# # Export DataFrame as CSV\n",
    "exported_df_minCols = workspace_dir + \"/\" + r\"exported_df_minCols.csv\"\n",
    "df_PE_calc.to_csv(exported_df_minCols, columns=DAsumDR_cols, float_format='%.3f')\n",
    "print(\"DataFrame exported as csv.\")\n",
    "\n",
    "# Convert the DataFrame CSV file to ArcGIS table\n",
    "inTable = exported_df_minCols\n",
    "outLocation = workspace\n",
    "outTable = \"exported_df_minCols_table\"\n",
    "try:\n",
    "    arcpy.TableToTable_conversion(inTable, outLocation, outTable)\n",
    "    print(\"DataFrame csv converted to ArcGIS table.\")\n",
    "except:\n",
    "    print(\"ArcGIS table already exists... deleting and trying again!\")\n",
    "    arcpy.Delete_management(outTable)\n",
    "    arcpy.TableToTable_conversion(inTable, outLocation, outTable)\n",
    "    print(\"DataFrame csv converted to ArcGIS table!\")\n",
    "\n",
    "# Join DataFrame table to PE_Grid\n",
    "inFeatures = PE_Grid\n",
    "joinField = \"LG_index\"\n",
    "joinTable = outTable\n",
    "fieldList = list(DAsumDR_cols) \n",
    "# fieldList.insert(0, \"LG_index\")\n",
    "print(\"Joining\", joinTable, \"to\", PE_Grid)\n",
    "arcpy.JoinField_management(inFeatures, joinField, joinTable, joinField, fieldList)\n",
    "\n",
    "print(\"DataFrame exported to\", workspace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting df to ArcGIS table...\n",
      "ArcGIS table already exists... deleting and trying again.\n",
      "DataFrame csv converted to ArcGIS table!\n",
      "Joining df_table to C:\\Users\\creasonc\\REE_PE_Script_dev\\11-1-19/REE_EnrichmentDatabase_PRB_DA_DS.gdb/PE_Grid_calc\n",
      "DataFrame exported to C:\\Users\\creasonc\\REE_PE_Script_dev\\11-1-19/REE_EnrichmentDatabase_PRB_DA_DS.gdb\n",
      "Converting df to ArcGIS table...\n",
      "ArcGIS table already exists... deleting and trying again.\n",
      "DataFrame csv converted to ArcGIS table!\n",
      "Joining df_table to PE_Grid_output\n",
      "DataFrame exported to C:\\Users\\creasonc\\REE_PE_Script_dev\\11-1-19/REE_EnrichmentDatabase_PRB_DA_DS.gdb\n",
      "Runtime: 364.53 seconds\n",
      "Runtime: 6.08 minutes\n",
      "Runtime: 0.1 hours\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Export dataframes to csv and ArcGIS tables, Join to PE_Grid_calc and PE_Grid_output \"\"\"\n",
    "\n",
    "### TESTED AND SUCCESSFUL ###\n",
    "\n",
    "################################################################################################\n",
    "def df_to_fc(df, cols, outLocation, outFile_csv, outFile_table, inFeatures):\n",
    "    \"\"\"\n",
    "    Export dataframes to csv and ArcGIS tables, and Join Fields to a Feature Class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: <str>\n",
    "        Name of the DataFrame to be converted to an ArcGIS Table\n",
    "        \n",
    "    cols: <list>\n",
    "        Columns to export to the Table and subsequently Join to the Feature Class\n",
    "        \n",
    "    outFile_csv: <str>\n",
    "        Filename for the csv output file (include \".csv\" extension)\n",
    "        \n",
    "    outFile_table: <str>\n",
    "        Filename for the ArcGIS Table output file (no extension necessary)\n",
    "        \n",
    "    outLocation: <str>\n",
    "        Directory where the csv and Table will be exported\n",
    "        \n",
    "    inFeatures: <str>\n",
    "        Feature Class to which the DataFrame columns will be joined\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    none\n",
    "        Only performs an operation; nothing to return.\n",
    "    \"\"\"\n",
    "\n",
    "    # Export DataFrame as CSV\n",
    "    localtime = time.asctime(time.localtime())  # Append localtime to filename\n",
    "    df_csv = workspace_dir + \"\\\\\" + outFile_csv + localtime.replace(\"  \", \" \").replace(\":\", \"_\") + r\".csv\"\n",
    "    df.to_csv(df_csv, columns=cols, float_format='%.3f')\n",
    "\n",
    "    # Convert the DataFrame CSV file to ArcGIS table\n",
    "    try:\n",
    "        print(\"Converting df to ArcGIS table...\")\n",
    "        arcpy.TableToTable_conversion(df_csv, outLocation, outFile_table)\n",
    "        print(\"DataFrame csv converted to ArcGIS table.\")\n",
    "    except:\n",
    "        print(\"ArcGIS table already exists... deleting and trying again.\")\n",
    "        arcpy.Delete_management(outFile_table)\n",
    "        arcpy.TableToTable_conversion(df_csv, outLocation, outFile_table)\n",
    "        print(\"DataFrame csv converted to ArcGIS table!\")\n",
    "    \n",
    "    # Join ArcGIS table to PE_Grid\n",
    "    joinField = \"LG_index\"\n",
    "    fieldList = list(cols) \n",
    "    print(\"Joining\", outFile_table, \"to\", inFeatures)\n",
    "    arcpy.JoinField_management(inFeatures, joinField, outFile_table, joinField, fieldList)\n",
    "\n",
    "    print(\"DataFrame exported to\", workspace)\n",
    "\n",
    "    return \n",
    "#########################################################################################################\n",
    "\n",
    "t_start = process_time()  # track processing time\n",
    "\n",
    "#  Add DA_sum_DR results to PE_Grid_calc\n",
    "df_to_fc(df=df_PE_calc, cols=DAsumDR_cols, outLocation=workspace, outFile_csv=FeatureDataset+'_df_export ', \n",
    "         outFile_table='df_table', inFeatures=PE_Grid)\n",
    "\n",
    "#  Add DA_sum_DR results to PE_Grid_calc\n",
    "df_to_fc(df=df_PE_calc, cols=DAsumDR_cols, outLocation=workspace, outFile_csv=FeatureDataset+'df_export ', \n",
    "         outFile_table='df_table', inFeatures='PE_Grid_output')\n",
    "\n",
    "\n",
    "# Print processing time\n",
    "t_stop = process_time()\n",
    "seconds = t_stop-t_start\n",
    "minutes = seconds/60\n",
    "hours = minutes/60\n",
    "\n",
    "print(\"Runtime:\", round(seconds, 2), \"seconds\")\n",
    "print(\"Runtime:\", round(minutes, 2), \"minutes\")\n",
    "print(\"Runtime:\", round(hours, 2), \"hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" NEXT STEPS \"\"\"\n",
    "\n",
    "# DONE 1) Copy DataFrame (df_PE_calc) to CSV and ESRI Table.  Will need to add DataFrame index as column to support merging.\n",
    "# See commented out code from Step 3 for guide.\n",
    "\n",
    "# DONE 2) Join ESRI Table to PE_Grid (and/or 'PE_Grid_clean').\n",
    "\n",
    "# 3) Test entire script from scratch.  Verify output in ArcPro.\n",
    "\n",
    "# 4) Adapt and test for DS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE FOR DEVELOPMENT ONLY ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDO-CODE FOR IMPLICIT PE SCORE CALCULATION #\n",
    "\n",
    "### Primary\n",
    "CID16 = CID10 and (CID14 or CID15)  # Eo\n",
    "\n",
    "CID11 = CID01 or CID02 or CID03 or CID04 or CID05 or CID06  # Fl\n",
    "CID12 = CID07 or CID08 or CID09  # Fl\n",
    "CID13 = CID10 or CID11 or CID12  # Fl\n",
    "CID19 = CID13 and CID17 and CID18  # Fl\n",
    "\n",
    "CID21 = (CID16 or CID19) and CID20  # Eo/Fl\n",
    "CID23 = CID21  # Eo/Fl\n",
    "\n",
    "PrimaryEnrichment = CID21 and CID23\n",
    "\n",
    "\n",
    "### Secondary\n",
    "CID33 = CID37 or CID38 or CID39 or CID40 or CID41\n",
    "CID34 = CID01 or CID02 or CID03 or CID04 or CID05 or CID06\n",
    "CID35 = CID30 or CID31 or CID32\n",
    "CID36 = CID33 or CID34 or CID35\n",
    "\n",
    "##### Meteoric\n",
    "CID50 = CID47 or CID48 or CID49  # MA/MP\n",
    "CID44 = CID42 or CID43  # MA/MP\n",
    "CID51 = CID36 and CID50 and CID44  # MA/MP\n",
    "CID53 = CID44 and CID51 and CID52  # MA/MP\n",
    "CID55 = CID53  # MP\n",
    "CID58 = (CID55 and CID56) or CID57  # MP\n",
    "CID59 = CID54 and CID53  # MA\n",
    "SecondaryEnrichmentMeteoric = CID58 or CID59\n",
    "\n",
    "##### Hydrothermal\n",
    "CID45 = CID42 or CID43  # HA/HP\n",
    "CID51 = CID36 and (CID45 or CID50)  # HA/HP\n",
    "CID53 = (CID45 or CID51) and CID52  # HA/HP\n",
    "CID55 = CID53  # HP\n",
    "CID46 = CID45  # HP\n",
    "CID58 = CID55 and CID56 or CID57 or CID46  # HP\n",
    "CID59 - CID53 and CID54  # HA\n",
    "SecondaryEnrichmentHydrothermal = CID58 or CID59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_cols.fillna(value={'LD' + '_index': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEVELOPMENT ONLY ###\n",
    "\n",
    "# Example code for updating contents of a dataframe while iterating row by row\n",
    "df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]},\n",
    "                  index=['dog', 'hawk'])\n",
    "for index_label, row_series in df.iterrows():\n",
    "    # For each row update the 'num_wings' value by 100x using df.at[]\n",
    "    df.at[index_label, 'num_wings'] = row_series['num_wings'] * 100\n",
    "    print('index_label:', index_label, \"\\n\")\n",
    "    print('row_series:', row_series, '\\n')\n",
    "    print('\\n')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_label, row_series in test.iterrows():\n",
    "    # For each row update the 'num_wings' value by 100x using df.at[]\n",
    "    df.at[index_label, 'num_wings'] = row_series['num_wings'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PE_calc.drop(axis=1,columns=['DA_Eo', 'DA_Fl', 'DA_HA', 'DA_HP', 'DA_MA', 'DA_MP', 'new', 'new1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ITERATE THROUGH DATAFRAME ROWS, CALCULATE MECHANISM SPECFIC TALLIES ###\n",
    "\n",
    "### DEVELOPMENT ONLY ###\n",
    "\n",
    "### TESTED AND SUCCESSFUL ###\n",
    "\n",
    "# df_PE_calc['new'].apply(lambda x: df_PE_calc['DA_Eo_LD_CID10'][x] + df_PE_calc['DA_Eo_LD_CID16'][x])  # can use with '+', 'or', 'and'\n",
    "# df_PE_calc['new'].apply(lambda x: sum([df_PE_calc['DA_Eo_LD_CID10'][x], df_PE_calc['DA_Eo_LD_CID16'][x]]))\n",
    "\n",
    "df_PE_calc['new1'] = 0\n",
    "PE_Eo_sum = (lambda x: sum([df_PE_calc['DA_Eo_LD_CID10'][x], df_PE_calc['DA_Eo_LD_CID16'][x]]))\n",
    "df_PE_calc['new1'].apply(PE_Eo_sum)\n",
    "\n",
    "# Alternative calculation\n",
    "df_PE_calc['new2'] = df_PE_calc[['DA_Eo_LD_CID10', 'DA_Eo_LD_CID16']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanismComponentsALL['Fl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanismComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = True\n",
    "b = False\n",
    "\n",
    "test = [(a or b), (a), b]\n",
    "\n",
    "sum(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = False\n",
    "b = True\n",
    "c = True\n",
    "d = numpy.nan\n",
    "e = None\n",
    "\n",
    "out = d and (b or c)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary\n",
    "# a = [1, 2, 3, 4, 5, 6] == [11]\n",
    "# [7:9] == [12]\n",
    "# [10:12] == [13]a\n",
    "# [14, 15] + [10] == [16]\n",
    "\n",
    "a = False\n",
    "b = True\n",
    "c = True\n",
    "d = False\n",
    "\n",
    "out = a + b + c + d\n",
    "\n",
    "print(out)\n",
    "\n",
    "\n",
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]},\n",
    "                  index=['dog', 'hawk'])\n",
    "print(df.dtypes,'\\n')\n",
    "print(df.astype('bool'),'\\n')\n",
    "print(df.astype('bool').dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dx_sum_fields\n",
    "mechanismComponents\n",
    "# [i for i in components_data_array if \"Eo\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Add DR fields for each emplacement type \"\"\"\n",
    "\n",
    "# Create DR fields in the DataFrame\n",
    "# DR_fields = [\"DR_\" + i for i in mechanismTypes]\n",
    "DR_values = {'DR_Eo': 7, 'DR_Fl': 8, 'DR_HA': 9, 'DR_HP': 10, 'DR_MA': 8, 'DR_MP': 10}\n",
    "\n",
    "# Populate DR fields in the DataFrame\n",
    "for i in DR_values:\n",
    "    df_PE_Score_calc[i] = DR_values[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF SCRIPT.  CELLS BELOW ARE FOR DEVELOPMENT ONLY."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### CODE ALREADY ADDED TO OTHER CELLS ###\n",
    "\n",
    "\"\"\" Calculate PE score (explicit tally of components; not implicit score)\"\"\"\n",
    "\n",
    "### THIS CODE WILL BE IN FINAL SCRIPT ###\n",
    "\n",
    "### TESTED AND SUCCESSFUL ###\n",
    "\n",
    "# df_PE_calc = df_dict_PostProcessed['compiled']\n",
    "\n",
    "############################################################################################################\n",
    "### Generic assignment for all cells (DA)\n",
    "df_PE_calc['DA_Eo_NT_CID20'] = True  # Accumulation of peat\n",
    "df_PE_calc['DA_Fl_NT_CID20'] = True  # Accumulation of peat\n",
    "\n",
    "df_PE_calc['DA_Fl_NT_CID22'] = True  # Burial of peat\n",
    "\n",
    "df_PE_calc['DA_Fl_NT_CID23'] = True  # Conversion of peat to coal\n",
    "\n",
    "df_PE_calc['DA_HA_LG_CID52'] = True # Coal and/or related strata\n",
    "df_PE_calc['DA_HP_LG_CID52'] = True # Coal and/or related strata\n",
    "df_PE_calc['DA_MA_LG_CID52'] = True # Coal and/or related strata\n",
    "df_PE_calc['DA_MP_LG_CID52'] = True # Coal and/or related strata\n",
    "\n",
    "### Powder River Basin assignment for all cells (DA)\n",
    "df_PE_calc['DA_Eo_LG_CID14'] = True  # Mire downwind of volcanism (this is true for PRB)\n",
    "df_PE_calc['DA_Fl_LD_CID17'] = True  # Mire in same paleo-drainage basin\n",
    "df_PE_calc['DA_Fl_LG_CID18'] = True  # Mire downstream of REE source\n",
    "############################################################################################################\n",
    "\n",
    "### Eo relevant components.  Not testable: CID15, CID21\n",
    "\n",
    "\n",
    "### Fl relevant components.  Not testable: CID17, CID18, CID19, CID21\n",
    "df_PE_calc['DA_Fl_NE_CID11'] = df_PE_calc[['DA_Fl_LD_CID01', 'DA_Fl_LD_CID02', 'DA_Fl_LD_CID03', 'DA_Fl_LD_CID04', \n",
    "                                           'DA_Fl_LD_CID05', 'DA_Fl_LD_CID06']].max(axis=1)  # Bedrock REE deposit\n",
    "df_PE_calc['DA_Fl_NE_CID12'] = df_PE_calc[['DA_Fl_LD_CID07', 'DA_Fl_LD_CID08', 'DA_Fl_LD_CID09']].max(axis=1)  # Sed REE deposit\n",
    "df_PE_calc['DA_Fl_NE_CID13'] = df_PE_calc[['DA_Fl_LD_CID10', 'DA_Fl_NE_CID11', 'DA_Fl_NE_CID12']].max(axis=1)  # REE source\n",
    "\n",
    "\n",
    "### HA relevant components.  Not testable: CID47, CID48, CID49, CID51, CID53, CID59\n",
    "df_PE_calc['DA_HA_NE_CID33'] = df_PE_calc[['DA_HA_UD_CID37', 'DA_HA_UD_CID38', 'DA_HA_UD_CID39', \n",
    "                                           'DA_HA_UD_CID40', 'DA_HA_UD_CID41']].max(axis=1)  # Alkaline volcanic ash\n",
    "df_PE_calc['DA_HA_NE_CID34'] = df_PE_calc[['DA_HA_LD_CID24', 'DA_HA_LD_CID25', 'DA_HA_LD_CID26',\n",
    "                                           'DA_HA_LD_CID27', 'DA_HA_LD_CID28', 'DA_HA_LD_CID29']].max(axis=1)  # Bedrock REE deposit\n",
    "df_PE_calc['DA_HA_NE_CID35'] = df_PE_calc[['DA_HA_LD_CID30', 'DA_HA_LD_CID31', 'DA_HA_LD_CID32']].max(axis=1)  # Sed REE deposit\n",
    "df_PE_calc['DA_HA_NE_CID36'] = df_PE_calc[['DA_HA_NE_CID33', 'DA_HA_NE_CID34', 'DA_HA_NE_CID35']].max(axis=1)  # REE source\n",
    "df_PE_calc['DA_HA_NE_42_43'] = df_PE_calc[['DA_HA_LG_CID42', 'DA_HA_UD_CID43']].max(axis=1)  # Conduit for fluid flow\n",
    "\n",
    "\n",
    "### HP relevant components.  Not testable: CID47, CID48, CID49, CID51, CID53, CID55, CID58\n",
    "df_PE_calc['DA_HP_NE_CID33'] = df_PE_calc[['DA_HP_UD_CID37', 'DA_HP_UD_CID38', 'DA_HP_UD_CID39', \n",
    "                                           'DA_HP_UD_CID40', 'DA_HP_UD_CID41']].max(axis=1)  # Alkaline volcanic ash\n",
    "df_PE_calc['DA_HP_NE_CID34'] = df_PE_calc[['DA_HP_LD_CID24', 'DA_HP_LD_CID25', 'DA_HP_LD_CID26',\n",
    "                                           'DA_HP_LD_CID27', 'DA_HP_LD_CID28', 'DA_HP_LD_CID29']].max(axis=1)  # Bedrock REE deposit\n",
    "df_PE_calc['DA_HP_NE_CID35'] = df_PE_calc[['DA_HP_LD_CID30', 'DA_HP_LD_CID31', 'DA_HP_LD_CID32']].max(axis=1)  # Sed REE deposit\n",
    "df_PE_calc['DA_HP_NE_CID36'] = df_PE_calc[['DA_HP_NE_CID33', 'DA_HP_NE_CID34', 'DA_HP_NE_CID35']].max(axis=1)  # REE source\n",
    "df_PE_calc['DA_HP_NE_42_43'] = df_PE_calc[['DA_HP_LG_CID42', 'DA_HP_UD_CID43']].max(axis=1)  # Conduit for fluid flow\n",
    "df_PE_calc['DA_HP_NE_57_46'] = df_PE_calc[['DA_HP_LG_CID57', 'DA_HP_LG_CID46']].max(axis=1)  # Dissolve phosphorus\n",
    "\n",
    "\n",
    "### MA relevant components.  Not testable:  CID44, CID47, CID48, CID49, CID51, CID53, CID59\n",
    "df_PE_calc['DA_MA_NE_CID33'] = df_PE_calc[['DA_MA_UD_CID37', 'DA_MA_UD_CID38', 'DA_MA_UD_CID39', \n",
    "                                           'DA_MA_UD_CID40', 'DA_MA_UD_CID41']].max(axis=1)  # Alkaline volcanic ash\n",
    "df_PE_calc['DA_MA_NE_CID34'] = df_PE_calc[['DA_MA_LD_CID24', 'DA_MA_LD_CID25', 'DA_MA_LD_CID26',\n",
    "                                           'DA_MA_LD_CID27', 'DA_MA_LD_CID28', 'DA_MA_LD_CID29']].max(axis=1)  # Bedrock REE deposit\n",
    "df_PE_calc['DA_MA_NE_CID35'] = df_PE_calc[['DA_MA_LD_CID30', 'DA_MA_LD_CID31', 'DA_MA_LD_CID32']].max(axis=1)  # Sed REE deposit\n",
    "df_PE_calc['DA_MA_NE_CID36'] = df_PE_calc[['DA_MA_NE_CID33', 'DA_MA_NE_CID34', 'DA_MA_NE_CID35']].max(axis=1)  # REE source\n",
    "df_PE_calc['DA_MA_NE_42_43'] = df_PE_calc[['DA_MA_LG_CID42', 'DA_MA_UD_CID43']].max(axis=1)  # Conduit for fluid flow\n",
    "\n",
    "\n",
    "### MP relevant components.  Not testable: CID47, CID48, CID49, CID51, CID53, CID55, CID58\n",
    "df_PE_calc['DA_MP_NE_CID33'] = df_PE_calc[['DA_MP_UD_CID37', 'DA_MP_UD_CID38', 'DA_MP_UD_CID39', \n",
    "                                           'DA_MP_UD_CID40', 'DA_MP_UD_CID41']].max(axis=1)  # Alkaline volcanic ash\n",
    "df_PE_calc['DA_MP_NE_CID34'] = df_PE_calc[['DA_MP_LD_CID24', 'DA_MP_LD_CID25', 'DA_MP_LD_CID26',\n",
    "                                           'DA_MP_LD_CID27', 'DA_MP_LD_CID28', 'DA_MP_LD_CID29']].max(axis=1)  # Bedrock REE deposit\n",
    "df_PE_calc['DA_MP_NE_CID35'] = df_PE_calc[['DA_MP_LD_CID30', 'DA_MP_LD_CID31', 'DA_MP_LD_CID32']].max(axis=1)  # Sed REE deposit\n",
    "df_PE_calc['DA_MP_NE_CID36'] = df_PE_calc[['DA_MP_NE_CID33', 'DA_MP_NE_CID34', 'DA_MP_NE_CID35']].max(axis=1)  # REE source\n",
    "df_PE_calc['DA_MP_NE_42_43'] = df_PE_calc[['DA_MP_LG_CID42', 'DA_MP_UD_CID43']].max(axis=1)  # Conduit for fluid flow\n",
    "\n",
    "############################################################################################################\n",
    "# DR components (NOTE: this is NOT the entire list of DR components; only those that are considered testable)\n",
    "DR_Eo = ['DA_Eo_LD_CID10', 'DA_Eo_LG_CID14', 'DA_Eo_LD_CID16', 'DA_Fl_NT_CID22', 'DA_Fl_NT_CID23']\n",
    "DR_Fl = ['DA_Fl_NE_CID13', 'DA_Fl_NT_CID20', 'DA_Fl_NT_CID22', 'DA_Fl_NT_CID23']\n",
    "DR_HA = ['DA_HA_NE_42_43', 'DA_HA_LG_CID52', 'DA_HA_NE_CID36', 'DA_HA_UD_CID45', 'DA_HA_LG_CID50', 'DA_HA_LG_CID54']\n",
    "DR_HP = ['DA_HP_NE_42_43', 'DA_HP_LG_CID52', 'DA_HP_NE_CID36', 'DA_HP_UD_CID45', 'DA_HP_LG_CID50', 'DA_HP_LG_CID56', 'DA_HP_NE_57_46']\n",
    "DR_MA = ['DA_MA_NE_42_43', 'DA_MA_LG_CID52', 'DA_MA_NE_CID36', 'DA_MA_LG_CID50', 'DA_MA_LG_CID54']\n",
    "DR_MP = ['DA_MP_NE_42_43', 'DA_MP_LG_CID52', 'DA_MP_NE_CID36', 'DA_MP_LG_CID50', 'DA_MP_LG_CID56']\n",
    "############################################################################################################\n",
    "\n",
    "# Add sum fields to dataframe\n",
    "df_PE_calc['Eo_sum'] = df_PE_calc[DR_Eo].sum(axis=1)\n",
    "df_PE_calc['Fl_sum'] = df_PE_calc[DR_Fl].sum(axis=1)\n",
    "df_PE_calc['HA_sum'] = df_PE_calc[DR_HA].sum(axis=1)\n",
    "df_PE_calc['HP_sum'] = df_PE_calc[DR_HP].sum(axis=1)\n",
    "df_PE_calc['MA_sum'] = df_PE_calc[DR_MA].sum(axis=1)\n",
    "df_PE_calc['MP_sum'] = df_PE_calc[DR_MP].sum(axis=1)\n",
    "\n",
    "# Display DA_sums\n",
    "DAsum_cols = []\n",
    "for mech in mechanismTypes:\n",
    "    DAsum_cols.append(mech + '_sum')\n",
    "df_PE_calc[DAsum_cols].describe()\n",
    "    \n",
    "# Calculate DA_sum/DR\n",
    "DAsumDR_cols = []\n",
    "for mech in mechanismTypes:\n",
    "    col = FeatureDataset + '_' + mech + '_sum_DR'\n",
    "    df_PE_calc[col] = df_PE_calc[mech + '_sum'] / len(\"DR_\" + mech)\n",
    "    DAsumDR_cols.append(col)\n",
    "df_PE_calc[DAsumDR_cols].describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### CODE ADDED TO OTHER CELLS ###\n",
    "\n",
    "\"\"\" Calculate DA step 4 of 4: Calculate sum(DA) for each REE emplacement type \"\"\"\n",
    "\n",
    "### IN DEVELOPMENT: ADDING CODE THAT SUMS DA FOR DIFFERENT SPATIAL TYPES\n",
    "\n",
    "# Create dataframe from PE_Grid\n",
    "    # Read in PE_Grid.  At this stage in the processing, PE_Grid will have DA for components for individual grids\n",
    "    # and for the domain extent (where applicable). In this section of code unnecessary fields in PE_Grid will be \n",
    "    # removed.\n",
    "    \n",
    "# # Read into a single dataframe all fields from each domainType (key) in df_dict_LG_domains_ALL\n",
    "# df_PE_Score_calc = df_dict_LG_domains_ALL['LG'].copy()\n",
    "# for domType in domainTypes:\n",
    "#     df_PE_Score_calc = df_PE_Score_calc.merge(df_dict_LG_domains_ALL[domType])\n",
    "    \n",
    "# Create a copy of the DataFrame that contains post-processed tallies.  \n",
    "df_keys = df_dict_LG_domains_ALL.keys()\n",
    "df_dict_PostProcessed = {}\n",
    "for key in df_keys:\n",
    "    df_dict_PostProcessed[key] = df_dict_LG_domains_ALL[key].copy()\n",
    "\n",
    "# Rename fields to simplified component names (e.g., remove \"_distributed\", etc.)\n",
    "sel = df_dict_PostProcessed['compiled'].columns  # list of fields\n",
    "rename_fields = [i for i in sel if \"_distributed\" in i]  # list of fields to be renamed\n",
    "for i in rename_fields:\n",
    "    df_dict_PostProcessed['compiled'].rename(columns={i:(i[:14])}, inplace=True)\n",
    "\n",
    "# Create dict of unique_components for each emplacement mechanism\n",
    "mechanismTypes = ['Eo', 'Fl', 'HA', 'HP', 'MA', 'MP']\n",
    "mechanismComponents = {}\n",
    "for mechanism in mechanismTypes:\n",
    "    mechanismComponents[mechanism]= [i for i in unique_components if mechanism in i]\n",
    "    \n",
    "    \n",
    "###    \n",
    "### THE CODE BELOW NEEDS TO BE VERIFIED; IT IS NOT GIVING THE CORRECT OUTPUT (DA IS GREATER THAN DR...) ####\n",
    "### \n",
    "\n",
    "\n",
    "# For each emplacement mechanism, sum the calculated components \n",
    "FeatureDataset=\"DA\"  # this variable is declared in the very first cell\n",
    "Dx_sum_fields = [FeatureDataset + '_' + i for i in mechanismTypes]\n",
    "for field in Dx_sum_fields:\n",
    "    df_dict_PostProcessed['compiled'][field] = df_dict_PostProcessed['compiled'][mechanismComponents[field[3:]]].sum(axis=1)  \n",
    "\n",
    "df_dict_PostProcessed['compiled'][Dx_sum_fields].describe()\n",
    "\n",
    "# Export 6 new sum(DA) fields, merge into copy of PE_Grid_clean (name it \"PE_Grid_FINAL\")\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\"\"\" Calculate DA step 3 of 4: Distribute DA across appropriate domain areas.  Assigns presence/absesnce \n",
    "    for a dataset within a geologic domain \"\"\"\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL ###\n",
    "\n",
    "### THIS CELL NOT NEEDED IN THE FINAL SCRIPT; WAS COPIED AND MODIFIED INTO CELL THAT IS NOW IN USE ###\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "def FieldValues(table, field):\n",
    "    \"\"\"\n",
    "    Create a list of unique values from a field in a feature class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table: <str>\n",
    "        Name of the table or feature class\n",
    "        \n",
    "    field: <str>\n",
    "        Name of the field \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    unique_values: <list>\n",
    "        Field values \n",
    "    \"\"\"\n",
    "    # Create a cursor object for reading the table\n",
    "    cursor = arcpy.da.SearchCursor(table, [field]) # A cursor iterates over rows in table\n",
    "\n",
    "    # Create an empty list for unique values\n",
    "    unique_values = []\n",
    "\n",
    "    # Iterate through rows\n",
    "    for row in cursor:\n",
    "        unique_values.append(row[0])\n",
    "    \n",
    "    return unique_values\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# Create a list of local grid index values, then create DataFrame\n",
    "LG_index_values = FieldValues(PE_Grid,'LG_index')\n",
    "df_index_cols = pd.DataFrame(LG_index_values, columns = {'LG_index'})\n",
    "df_dict_domainALL = {\"LG\": df_index_cols}\n",
    "\n",
    "# Determine which domain types have available datasets for the AOI\n",
    "domainTypes_ALL = [\"LD\", \"SD\", \"SA\", \"UD\"]\n",
    "domainTypes = []\n",
    "for domainType in domainTypes_ALL:\n",
    "    count = [i for i in unique_components if domainType in i]\n",
    "    if len(count) > 0:\n",
    "        domainTypes.append(domainType)\n",
    "\n",
    "# Distribute presence across domain for each domain type\n",
    "for domainType in domainTypes:\n",
    "\n",
    "    print(domainType, \"started...\")\n",
    "    \n",
    "    # Create a list of domain index values (e.g, LD1, LD2, LD3, LD4), then add to DataFrame\n",
    "    domainType_index_values = FieldValues(PE_Grid, domainType + '_index')\n",
    "    df_index_cols[domainType + '_index'] = domainType_index_values\n",
    "\n",
    "    # Create a list of columns for each domain type (components with domain subtext (e.g., LD) in the name)\n",
    "    domainType_components = [i for i in unique_components if domainType in i]\n",
    "\n",
    "    # Create DataFrame with values for records in each domainType column\n",
    "    col = {}\n",
    "    for i in domainType_components:\n",
    "        col[i] = FieldValues(PE_Grid, i)\n",
    "    df_domainType_fieldvalues = pd.DataFrame(col)\n",
    "\n",
    "    # Join into a new DataFrame the LD_index and domainType_components\n",
    "    df_domainType_joined = df_index_cols.join(df_domainType_fieldvalues, sort=False)\n",
    "\n",
    "    # Group by unique domainType_index values\n",
    "    df_domainType_grouped = df_domainType_joined.groupby([domainType + '_index'])\n",
    "\n",
    "    # Determine max of DA for each domainType_index group, return in \"DA_...domainType..._domain\" column\n",
    "    df_domainType_max = df_domainType_grouped.max()\n",
    "    for i in domainType_components:\n",
    "        df_domainType_max.rename(columns={i:(i + \"_domain\")}, inplace=True)\n",
    "    df_domainType_max.drop(['LG_index'], axis=1, inplace=True)  # LG_index is erroneously overwritten without this line\n",
    "\n",
    "    # Merge index and DA_max columns in a new DataFrame\n",
    "    df_domainType_export = df_index_cols.merge(df_domainType_max, on = domainType + '_index')\n",
    "    # df_domainType_all = df_domainType_joined.merge(df_domainType_max, on = domainType + '_index')\n",
    "\n",
    "    # Combine all domain types into a list/dict of DataFrames\n",
    "#     df_domainALL.append(df_domainType_export)\n",
    "    df_dict_domainALL[domainType] = df_domainType_export\n",
    "\n",
    "    \n",
    "#     THIS SECTION COMMENTED OUT BECAUSE FILES ALREADY EXIST; JUST NEED TO RE-ESTABLISH THE VARIABLES\n",
    "\n",
    "#     # Export domainType DataFrame as CSV\n",
    "#     exported_df_domainType = workspace_dir + \"/\" + domainType + r\"_domains_exported_df.csv\"\n",
    "#     df_domainType_export.to_csv(exported_df_domainType, index=False)\n",
    "\n",
    "#     # Convert the DataFrame CSV file to ArcGIS table (if not already created)\n",
    "#     inTable = exported_df_domainType\n",
    "#     outLocation = workspace\n",
    "#     outTable = str(domainType + \"_domain_df_table\")\n",
    "#     try:\n",
    "#         arcpy.TableToTable_conversion(inTable, outLocation, outTable)\n",
    "#     except:\n",
    "#         print(str(domainType + \"_index\"), \"DataFrame csv already converted to ArcGIS table!\")\n",
    "    \n",
    "#     # Join DataFrame table to PE_Grid\n",
    "#     inFeatures = PE_Grid\n",
    "#     joinField = \"LG_index\"\n",
    "#     joinTable = outTable\n",
    "#     fieldList = list(df_domainType_export.columns) \n",
    "#     fieldList.remove(\"LG_index\")  # exclude indicies from fields to join \n",
    "#     for dT in domainTypes:\n",
    "#         try:\n",
    "#             fieldList.remove(str(dT + \"_index\")) # exclude indicies from fields to join\n",
    "#         except:\n",
    "#             print(\"removing unnecessary fields from Join list...\")\n",
    "#     arcpy.JoinField_management(inFeatures, joinField, joinTable, joinField, fieldList)\n",
    "    \n",
    "    print(domainType, \"finished\")\n",
    "\n",
    "print('\\nAll done.')\n",
    "\n",
    "# # number of cells with data in each LD domain\n",
    "# df_domainALL[1]['LD_index'].value_counts()\n",
    "\n",
    "# # number of cells with data in each UD domain\n",
    "# df_domainALL[2]['UD_index'].value_counts()\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Try single iteration of select intersection, generate new feature from selection, add 1 to selected records' DA field, join DA field to PE_Grid  \"\"\"  \n",
    "\n",
    "\"\"\" CREATES NEW FIELD FOR EACH COMPONENT \"\"\"\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL, BUT OVERWRITES DA IF THERE ARE MULTIPLE DATASETS FOR A COMPONENT... ###\n",
    "\n",
    "# delete selection layer from memory  ### NOTE: I think this is only needed for development; may not be not necessary for iteration ###\n",
    "# arcpy.Delete_management('memory\\DA_Eo_LD_CID10_selected')\n",
    "arcpy.Delete_management('DA_Eo_LD_CID10_selected')\n",
    "\n",
    "# Create new field with same name as component code prefix\n",
    "arcpy.AddField_management(PE_Grid, 'DA_Eo_LD_CID10', \"SHORT\")\n",
    "\n",
    "# Select rows in PE_Grid that intersect component[1]\n",
    "selection = arcpy.SelectLayerByLocation_management(in_layer=PE_Grid, overlap_type=\"INTERSECT\", select_features='DA_Eo_LD_CID10_USTRAT', search_distance=\"\", selection_type=\"NEW_SELECTION\", invert_spatial_relationship=\"NOT_INVERT\")\n",
    "# selection = arcpy.SelectLayerByLocation_management(PE_Grid, \"INTERSECT\", 'DA_Eo_LD_CID10_USTRAT', \"\", \"NEW_SELECTION\", \"NOT_INVERT\")\n",
    "\n",
    "# Create new layer from selection  ## \n",
    "selection_lyr = arcpy.CopyFeatures_management(in_features=selection, out_feature_class=\"DA_Eo_LD_CID10_selected\")\n",
    "# selection_lyr = arcpy.CopyFeatures_management(in_features=selection, out_feature_class=r\"memory\\DA_Eo_LD_CID10_selected\")\n",
    "# selection_lyr = arcpy.CopyFeatures_management(selection, r\"memory\\DA_Eo_LD_CID10_selected\")\n",
    "\n",
    "# Delete from PE_Grid the field with same name as component code prefix\n",
    "arcpy.DeleteField_management(in_table=PE_Grid, drop_field='DA_Eo_LD_CID10')\n",
    "\n",
    "# In selection, set Components[0] field to 1  ## Both of the lower lines work\n",
    "# calc_field = arcpy.CalculateField_management(in_table=selection_lyr, field='DA_Eo_LD_CID10', expression=\"5\", expression_type=\"PYTHON3\", code_block=\"\")\n",
    "calc_field = arcpy.CalculateField_management(selection_lyr, 'DA_Eo_LD_CID10', \"3\", \"PYTHON3\", \"\")\n",
    "# calc_field = arcpy.CalculateField_management(in_table=\"DA_Eo_LD_CID10_selected\", field='DA_Eo_LD_CID10', expression=\"10\", expression_type=\"PYTHON3\", code_block=\"\")\n",
    "\n",
    "\n",
    "# Join field to PE_Grid (add DA_component field from 'selection')  ## All three of the lines work\n",
    "join_field = arcpy.JoinField_management(in_data=PE_Grid, in_field=\"LG_index\", join_table=selection_lyr, join_field=\"LG_index\", fields='DA_Eo_LD_CID10')\n",
    "# arcpy.JoinField_management(in_data=PE_Grid, in_field=\"LG_index\", join_table=\"DA_Eo_LD_CID10_selected\", join_field=\"LG_index\", fields='DA_Eo_LD_CID10')\n",
    "# join_field = arcpy.JoinField_management(in_data=PE_Grid, in_field=\"LG_index\", join_table=\"DA_Eo_LD_CID10_selected\", join_field=\"LG_index\", fields='DA_Eo_LD_CID10')\n",
    "\n",
    "# Delete selection layer and variables from memory\n",
    "arcpy.Delete_management(selection_lyr)\n",
    "del(selection, selection_lyr, join_field, calc_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Try single iteration of select intersection,  generate new feature from selection, add 1 to selected records' DA field, join DA field to PE_Grid  \"\"\"  \n",
    "\n",
    "\"\"\"  CREATES NEW FIELD FOR EACH DATASET \"\"\"\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL ###\n",
    "\n",
    "# delete selection layer from memory  ### NOTE: I think this is only needed for development; may not be not necessary for iteration ###\n",
    "# arcpy.Delete_management('memory\\DA_Eo_LD_CID10_selected')\n",
    "arcpy.Delete_management('DA_Eo_LD_CID10_USTRAT_selected')\n",
    "\n",
    "# Create new field with same name as component code prefix and set to zero\n",
    "arcpy.AddField_management(PE_Grid, 'DA_Eo_LD_CID10_USTRAT', \"SHORT\")\n",
    "# arcpy.CalculateField_management(PE_Grid, 'DA_Eo_LD_CID10_USTRAT', \"0\", \"PYTHON3\", \"\")\n",
    "\n",
    "# Select rows in PE_Grid that intersect component[1]\n",
    "selection = arcpy.SelectLayerByLocation_management(in_layer=PE_Grid, overlap_type=\"INTERSECT\", select_features='DA_Eo_LD_CID10_USTRAT', search_distance=\"\", selection_type=\"NEW_SELECTION\", invert_spatial_relationship=\"NOT_INVERT\")\n",
    "# selection = arcpy.SelectLayerByLocation_management(PE_Grid, \"INTERSECT\", 'DA_Eo_LD_CID10_USTRAT', \"\", \"NEW_SELECTION\", \"NOT_INVERT\")\n",
    "\n",
    "# Create new layer from selection  ## \n",
    "selection_lyr = arcpy.CopyFeatures_management(in_features=selection, out_feature_class=\"DA_Eo_LD_CID10_USTRAT_selected\")\n",
    "# selection_lyr = arcpy.CopyFeatures_management(in_features=selection, out_feature_class=r\"memory\\DA_Eo_LD_CID10_selected\")\n",
    "# selection_lyr = arcpy.CopyFeatures_management(selection, r\"memory\\DA_Eo_LD_CID10_selected\")\n",
    "\n",
    "# Delete from PE_Grid the field with same name as component code prefix\n",
    "arcpy.DeleteField_management(in_table=PE_Grid, drop_field='DA_Eo_LD_CID10_USTRAT')\n",
    "\n",
    "# In selection, set Components[0] field to 1  ## Both of the lower lines work\n",
    "# calc_field = arcpy.CalculateField_management(in_table=selection_lyr, field='DA_Eo_LD_CID10', expression=\"5\", expression_type=\"PYTHON3\", code_block=\"\")\n",
    "calc_field = arcpy.CalculateField_management(selection_lyr, 'DA_Eo_LD_CID10_USTRAT', \"1\", \"PYTHON3\", \"\")\n",
    "# calc_field = arcpy.CalculateField_management(in_table=\"DA_Eo_LD_CID10_selected\", field='DA_Eo_LD_CID10', expression=\"10\", expression_type=\"PYTHON3\", code_block=\"\")\n",
    "\n",
    "\n",
    "# Join field to PE_Grid (add DA_component field from 'selection')  ## All three of the lines work\n",
    "join_field = arcpy.JoinField_management(in_data=PE_Grid, in_field=\"LG_index\", join_table=selection_lyr, join_field=\"LG_index\", fields='DA_Eo_LD_CID10_USTRAT')\n",
    "# arcpy.JoinField_management(in_data=PE_Grid, in_field=\"LG_index\", join_table=\"DA_Eo_LD_CID10_selected\", join_field=\"LG_index\", fields='DA_Eo_LD_CID10')\n",
    "# join_field = arcpy.JoinField_management(in_data=PE_Grid, in_field=\"LG_index\", join_table=\"DA_Eo_LD_CID10_selected\", join_field=\"LG_index\", fields='DA_Eo_LD_CID10')\n",
    "\n",
    "\n",
    "# Delete selection layer and variables from memory\n",
    "arcpy.Delete_management(selection_lyr)\n",
    "del(selection, selection_lyr, join_field, calc_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Try single iteration of select intersection -USING MEMORY- generate new feature from selection, add 1 to selected records' DA field, join DA field to PE_Grid  \"\"\"  \n",
    "\n",
    "\"\"\" CREATES NEW FIELD FOR EACH COMPONENT \"\"\"\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL, ALTHOUGH TAKES LONGER TO EXECUTE THAN NOT LOADING INTO MEMORY ###\n",
    "\n",
    "# delete selection layer from memory  ### NOTE: I think this is only needed for development; may not be not necessary for iteration ###\n",
    "arcpy.Delete_management('memory\\DA_Eo_LD_CID10_selected')\n",
    "arcpy.Delete_management('memory\\DA_Eo_LD_CID10_USTRAT')\n",
    "arcpy.Delete_management('memory\\PE_Grid_memory')\n",
    "arcpy.Delete_management('DA_Eo_LD_CID10_selected')\n",
    "arcpy.Delete_management('PE_Grid_out')\n",
    "\n",
    "# load features to memory\n",
    "PE_Grid_memory = arcpy.CopyFeatures_management(PE_Grid, r\"memory\\PE_Grid_memory\")\n",
    "feature_memory = arcpy.CopyFeatures_management('DA_Eo_LD_CID10_USTRAT', r'memory\\DA_Eo_LD_CID10_USTRAT')\n",
    "\n",
    "# Create new field with same name as component code prefix\n",
    "arcpy.AddField_management(PE_Grid_memory, 'DA_Eo_LD_CID10', \"SHORT\")\n",
    "\n",
    "\n",
    "# Select rows in PE_Grid that intersect component[1]\n",
    "selection = arcpy.SelectLayerByLocation_management(in_layer=PE_Grid_memory, overlap_type=\"INTERSECT\", select_features=feature_memory, search_distance=\"\", selection_type=\"NEW_SELECTION\", invert_spatial_relationship=\"NOT_INVERT\")\n",
    "# selection = arcpy.SelectLayerByLocation_management(PE_Grid, \"INTERSECT\", 'DA_Eo_LD_CID10_USTRAT', \"\", \"NEW_SELECTION\", \"NOT_INVERT\")\n",
    "\n",
    "# Create new layer from selection  ## \n",
    "selection_lyr = arcpy.CopyFeatures_management(in_features=selection, out_feature_class=selection_lyr)\n",
    "# selection_lyr = arcpy.CopyFeatures_management(in_features=selection, out_feature_class=r\"memory\\DA_Eo_LD_CID10_selected\")\n",
    "# selection_lyr = arcpy.CopyFeatures_management(selection, r\"memory\\DA_Eo_LD_CID10_selected\")\n",
    "\n",
    "# Delete from PE_Grid the field with same name as component code prefix\n",
    "arcpy.DeleteField_management(in_table=PE_Grid_memory, drop_field='DA_Eo_LD_CID10')\n",
    "\n",
    "# In selection, set Components[0] field to 1  ## Both of the lower lines work\n",
    "# calc_field = arcpy.CalculateField_management(in_table=selection_lyr, field='DA_Eo_LD_CID10', expression=\"5\", expression_type=\"PYTHON3\", code_block=\"\")\n",
    "calc_field = arcpy.CalculateField_management(selection_lyr, 'DA_Eo_LD_CID10', \"3\", \"PYTHON3\", \"\")\n",
    "# calc_field = arcpy.CalculateField_management(in_table=\"DA_Eo_LD_CID10_selected\", field='DA_Eo_LD_CID10', expression=\"10\", expression_type=\"PYTHON3\", code_block=\"\")\n",
    "\n",
    "\n",
    "# Join field to PE_Grid (add DA_component field from 'selection')  ## All three of the lines work\n",
    "join_field = arcpy.JoinField_management(in_data=PE_Grid_memory, in_field=\"LG_index\", join_table=selection_lyr, join_field=\"LG_index\", fields='DA_Eo_LD_CID10')\n",
    "# arcpy.JoinField_management(in_data=PE_Grid, in_field=\"LG_index\", join_table=\"DA_Eo_LD_CID10_selected\", join_field=\"LG_index\", fields='DA_Eo_LD_CID10')\n",
    "# join_field = arcpy.JoinField_management(in_data=PE_Grid, in_field=\"LG_index\", join_table=\"DA_Eo_LD_CID10_selected\", join_field=\"LG_index\", fields='DA_Eo_LD_CID10')\n",
    "\n",
    "# Write PE_Grid_memory to disk\n",
    "PE_Grid_out = arcpy.CopyFeatures_management(in_features=PE_Grid_memory, out_feature_class=\"PE_Grid_out\")\n",
    "\n",
    "# Delete selection layer and variables from memory\n",
    "# arcpy.Delete_management(selection_lyr)\n",
    "# del(selection, selection_lyr, join_field, calc_field)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### THIS IS NOT READY YET -OUTPUT IS NOT VALID- NEED TO REFINE THE SINGLE ITERATION CODE BEFORE RETURING HERE ###\n",
    "\n",
    "\n",
    "# Iterate through features for each component, add new field with the code prefix, test for intersection with PE_Grid and features, add DA\n",
    "for component_datasets in components_data_array:\n",
    "    # Test for intersect between PE_Grid cells and data features\n",
    "    for feature_class in component_datasets:\n",
    "        \n",
    "#         # delete selection layer from memory  ### NOTE: I think this is only needed for development; may not be not necessary for iteration ###\n",
    "#         try:\n",
    "#             arcpy.Delete_management(new_layer_name)\n",
    "#             print(\"cleared existing 'new_layer_name' from memory\")\n",
    "#         except:\n",
    "#             print(\"nothing in memory to delete\")                \n",
    "        \n",
    "        # this variable is the component code prefix (e.g., DA_Eo_LD_CID16) at the current iteration step\n",
    "        component = unique_components[component_datasets.index(feature_class)]\n",
    "        \n",
    "        # Create new field with same name as component code prefix\n",
    "        arcpy.AddField_management(PE_Grid, component, \"SHORT\")\n",
    "        print(\"added field for component:\", component)\n",
    "        \n",
    "        # Select layer by location\n",
    "        selection = arcpy.SelectLayerByLocation_management(in_layer=PE_Grid, overlap_type=\"INTERSECT\", select_features=feature_class, search_distance=\"\", selection_type=\"NEW_SELECTION\", invert_spatial_relationship=\"NOT_INVERT\")\n",
    "        print(\"selected layer for component:\", component)\n",
    "       \n",
    "        # Create new layer from selection\n",
    "#         new_layer_name = 'memory/' + component + '_selected'\n",
    "        selection_lyr = arcpy.CopyFeatures_management(selection, component + \"_selected\")\n",
    "        print(\"copied layer for selection_lyr:\", selection_lyr)       \n",
    "       \n",
    "        # Delete from PE_Grid the field with same name as component code prefix\n",
    "        arcpy.DeleteField_management(in_table=PE_Grid, drop_field=component)\n",
    "        print(\"deleted field for component:\", component)\n",
    "        \n",
    "        # Set select field to 1 \n",
    "        calc_field = arcpy.CalculateField_management(in_table=selection_lyr, field=component, expression=\"5\", expression_type=\"PYTHON3\", code_block=\"\")\n",
    "        print(\"calculated field for component:\", component)\n",
    "        \n",
    "        # Join field to PE_Grid (add DA_component field from 'selection')\n",
    "        join_field = arcpy.JoinField_management(in_data=PE_Grid, in_field=\"LG_index\", join_table=selection_lyr, join_field=\"LG_index\", fields=component)\n",
    "        print(\"joined field for component:\", component, \"\\n\")\n",
    "        \n",
    "        # delete selection layer from memory\n",
    "        arcpy.Delete_management(selection_lyr)\n",
    "#         break\n",
    "    break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Side code for appending a dictionary -- NOT NEEDED FOR PE SCORE \"\"\"\n",
    "\n",
    "# d = {feature_class: dt}\n",
    "processing[feature_class]= dt\n",
    "d.update({\"xyz\": 1})\n",
    "d['abc']= 3\n",
    "print(d)\n",
    "\n",
    "# list keys in a dict\n",
    "d_new = {\"one\": 1, \"two\": 2, \"three\": 3}\n",
    "d_new.keys()\n",
    "\n",
    "# minval = max(d.values())\n",
    "# res = [k for k, v in d.items() if v==minval]\n",
    "\n",
    "# res = str(res)\n",
    "\n",
    "# print(res)\n",
    "\n",
    "# print(\"process with longest runtime:\", str(res), \"at\", processing.get('xyz'), \"seconds\")      "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CONSIDER MODIFYING TO UPLOAD feature_class TO MEMORY TO EXPEDITE EXECUTION TIME\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL.  INCORPORATED TO DA SCRIPT ### \n",
    "\n",
    "def replaceNULL(feature_class, field):\n",
    "    \"\"\"\n",
    "    Replace NULL values with zeros for a field in a feature class\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_class: <str>\n",
    "        Name of feature class containing the field to be modified\n",
    "    field: <str>\n",
    "        Name of the field to be evaluated and modified if necessary\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    None; this function only modifies the field in the feature_class\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create update cursor for feature class \n",
    "    with arcpy.da.UpdateCursor(feature_class, field) as cursor:\n",
    "        # For each row, evaluate field value and replace with zero if NULL\n",
    "        for row in cursor:\n",
    "            if row[0] == None:\n",
    "                row[0] = 0\n",
    "            else: \n",
    "                row[0] = row[0]\n",
    "            # Update the cursor with the updated list\n",
    "            cursor.updateRow(row)\n",
    "    return\n",
    "\n",
    "# Iterate through features for each component, add new field with the code prefix, test for intersection with PE_Grid and features, add DA\n",
    "for component_datasets in components_data_array:\n",
    "    # Test for intersect between PE_Grid cells and data features\n",
    "    for feature_class in component_datasets:\n",
    "        replaceNULL(PE_Grid, feature_class)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Delete stuff \"\"\"\n",
    "# This cell only needed for development\n",
    "\n",
    "# 'DA_Eo_LD_CID10', 'DA_Eo_LD_CID10_1', 'DA_Eo_LD_CID10_16', 'DA_Eo_LD_CID10_USTRAT', DA_Eo_LD_CID10_SGMC_Geology\n",
    "arcpy.DeleteField_management(in_table=PE_Grid, drop_field='DA_HA_UD_CID39')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print(\"unique_components[0:5]:\", unique_components[0:5], \"\\n\")\n",
    "# print(\"component_datasets:\", component_datasets, \"\\n\")\n",
    "print(\"components_data_array[0:5]:\", components_data_array[0:5], \"\\n\")\n",
    "# # print(\"unique_components:\", unique_components, \"\\n\")\n",
    "\n",
    "# field_names = ListFieldNames(PE_Grid)\n",
    "# print(\"PE_Grid attributes:\", field_names)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "field_names = ListFieldNames(PE_Grid)\n",
    "print(\"PE_Grid attributes:\", field_names, \"\\n\")\n",
    "\n",
    "# field_names = ListFieldNames(str(selection))\n",
    "# print(\"selection attributes:\", field_names, \"\\n\")\n",
    "\n",
    "# field_names = ListFieldNames(str(selection_lyr))\n",
    "# print(\"selection_lyr attributes:\", field_names, \"\\n\")\n",
    "\n",
    "# arcpy.CopyFeatures_management(in_features=calc_field, out_feature_class=\"calc_field\")\n",
    "\n",
    "# field_names = ListFieldNames(str(calc_field))\n",
    "# print(\"calc_field attributes:\", field_names, \"\\n\")\n",
    "\n",
    "# field_names = ListFieldNames(str(\"DA_Eo_LD_CID10_selected\"))\n",
    "# print(\"DA_Eo_LD_CID10_selected attributes:\", field_names, \"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del(selection, selection_lyr, join_field, calc_field)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\"\"\" Calculate DA for a single component \"\"\"\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL ###\n",
    "\n",
    "### HOWEVER, ONLY A SINGLE ITERATION, STILL NEEDS FUNTIONALITY ADDED TO ITERATE THROUGH ALL COMPONENTS ###\n",
    "\n",
    "######################################################################################################################\n",
    "def FieldValues(table, field):\n",
    "    \"\"\"\n",
    "    Create a list of unique values from a field in a feature class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table: <str>\n",
    "        Name of the table or feature class\n",
    "        \n",
    "    field: <str>\n",
    "        Name of the field \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    unique_values: <list>\n",
    "        Field values \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a cursor object for reading the table\n",
    "    cursor = arcpy.da.SearchCursor(table, [field]) # A cursor iterates over rows in table\n",
    "\n",
    "    # Create an empty list for unique values\n",
    "    unique_values = []\n",
    "\n",
    "    # Iterate through rows\n",
    "    for row in cursor:\n",
    "        unique_values.append(row[0])\n",
    "    \n",
    "    return unique_values\n",
    "######################################################################################################################\n",
    "\n",
    "fc = PE_Grid\n",
    "component = 12\n",
    "\n",
    "field_names = ListFieldNames(PE_Grid)  # Update field names\n",
    "\n",
    "# List containing the unique component and corresponding feature datasets (that are represented as fields in PE_Grid)\n",
    "component_fields =  [unique_components[component]] + components_data_array[component]\n",
    "\n",
    "# Create a new field for unique_component if it does not already exist\n",
    "present = 0\n",
    "for indiv_field in field_names:\n",
    "    if indiv_field == unique_components[component]:\n",
    "        present = present + 1\n",
    "    else:\n",
    "        present = present + 0\n",
    "if not present:\n",
    "    # Add new field for unique_component\n",
    "    arcpy.AddField_management(PE_Grid, unique_components[component], \"SHORT\")\n",
    "    print(\"Added new field for\", unique_components[component])\n",
    "#     arcpy.CalculateField_management(PE_Grid, unique_components[component], expression=\"0\", expression_type=\"PYTHON3\", code_block=\"\")\n",
    "#     print(\"New field was set to zero\")\n",
    "    # Assign DA value for each component\n",
    "    with arcpy.da.UpdateCursor(fc, component_fields) as cursor:\n",
    "        for row in cursor:\n",
    "    #         print('row[0] before:', row[0])\n",
    "    #         print('row before:', row)\n",
    "            row[0] = 0\n",
    "            row[0] = max(row)\n",
    "    #         print('row[0] after:', row[0])\n",
    "    #         print('row after:', row)\n",
    "            cursor.updateRow(row)  # Update the cursor with the updated list\n",
    "    #         break\n",
    "else:\n",
    "    print(\"Field already exists for\", unique_components[component], \"... skipped\")\n",
    "\n",
    "# print for development\n",
    "print('component_fields:', component_fields)\n",
    "\n",
    "        \n",
    "#  Create list of unique values from a field  ## This is for development; not needed for PE Score\n",
    "fv = FieldValues(PE_Grid, unique_components[component])\n",
    "# print(unique_components[component], \"Values:\", set(fv), \"  Sum:\", sum(fv))\n",
    "print(unique_components[component])\n",
    "print(\"Values:\", set(fv))\n",
    "print(\"Sum:\", sum(fv))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#  DEVELOPMENT ONLY -- Create list of unique values from a field\n",
    "    fv = FieldValues(PE_Grid, unique_components[i])\n",
    "    try:\n",
    "        print(unique_components[i], \"Values:\", set(fv), \"  Sum:\", sum(fv), \"\\n\")\n",
    "    except:\n",
    "        print(\"\\n\\n *** INCOMPLETE DA CALCULATION FOR\", unique_components[i], \"***\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Unique values from a field in a feature class \"\"\"\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL ###\n",
    "\n",
    "### FUNCTION ADDED TO CELL THAT DETERMINES DA FOR EACH COMPONENT ###\n",
    "\n",
    "i = 2\n",
    "\n",
    "def FieldValues(table, field):\n",
    "    \"\"\"\n",
    "    Create a list of unique values from a field in a feature class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table: <str>\n",
    "        Name of the table or feature class\n",
    "        \n",
    "    field: <str>\n",
    "        Name of the field \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    unique_values: <list>\n",
    "        Field values \n",
    "    \"\"\"\n",
    "    # Create a cursor object for reading the table\n",
    "    cursor = arcpy.da.SearchCursor(table, [field]) # A cursor iterates over rows in table\n",
    "\n",
    "    # Create an empty list for unique values\n",
    "    unique_values = []\n",
    "\n",
    "    # Iterate through rows\n",
    "    for row in cursor:\n",
    "        unique_values.append(row[0])\n",
    "    \n",
    "    return unique_values\n",
    "\n",
    "# unique = FieldValues(PE_Grid, 'DA_MP_UD_CID43_qfaults')\n",
    "fv = FieldValues(PE_Grid, unique_components[i])\n",
    "\n",
    "print(unique_components[i], \"Values:\", set(fv), \"  Sum:\", sum(fv))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DEVELOPMENT ONLY\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['unique_components'] = unique_components\n",
    "df['component_data_array'] = components_data_array\n",
    "\n",
    "print(df.iloc[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for calculating DA for domains, consider using sum() to add DA values for each domain.  Then, if DA sum() > 0, can assign the entire domain DA = 1.\n",
    "\n",
    "# CONSIDER CREATING A PANDAS DATAFRAME FOR THE PE_GRID ATTRIBUTE TABLE TO HELP ORGAINIZATION AND EXPEDITE CALCULATIONS..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\"\"\" Distribute DA across the different domains using a Pandas DataFrame\"\"\"\n",
    "\n",
    "### ONLY CONSIDERING LITHOLOGIC DOMAINS (LD) AT THIS POINT -- NEED TO ADD SD ONCE TESTED AND SUCCESSFUL ###\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL - NEED TO VALIDATE DOMAIN RESULTS... ###  \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "######################################################################################################################\n",
    "def FieldValues(table, field):\n",
    "    \"\"\"\n",
    "    Create a list of unique values from a field in a feature class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table: <str>\n",
    "        Name of the table or feature class\n",
    "        \n",
    "    field: <str>\n",
    "        Name of the field \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    unique_values: <list>\n",
    "        Field values \n",
    "    \"\"\"\n",
    "    # Create a cursor object for reading the table\n",
    "    cursor = arcpy.da.SearchCursor(table, [field]) # A cursor iterates over rows in table\n",
    "\n",
    "    # Create an empty list for unique values\n",
    "    unique_values = []\n",
    "\n",
    "    # Iterate through rows\n",
    "    for row in cursor:\n",
    "        unique_values.append(row[0])\n",
    "    \n",
    "    return unique_values\n",
    "######################################################################################################################\n",
    "\n",
    "# Set to zero the domain values for cells outside AOI (only need this for one instance; then turn off)\n",
    "# replaceNULL(PE_Grid,'LD_index')\n",
    "\n",
    "# Create a list of local grid index values, then create DataFrame\n",
    "LG_index_values = FieldValues(PE_Grid,'LG_index')\n",
    "# LD_values_unique = set(FieldValues(PE_Grid,'LD_index'))\n",
    "df_index_cols = pd.DataFrame(LG_index_values, columns = {'LG_index'})\n",
    "\n",
    "# Create a list of domain index values (e.g, LD1, LD2, LD3, LD4), then add to DataFrame\n",
    "LD_index_values = FieldValues(PE_Grid,'LD_index')\n",
    "    # LD_values_unique = set(FieldValues(PE_Grid,'LD_index'))\n",
    "    # df_LD_index_col = pd.DataFrame(LD_index_values, columns = {'LD_index'})\n",
    "    # SD_values = FieldValues(PE_Grid,'SD_index')\n",
    "    # SD_values_unique = set(FieldValues(PE_Grid,'SD_index'))\n",
    "    # UD_values = FieldValues(PE_Grid,'UD_index')\n",
    "    # UD_values_unique = set(FieldValues(PE_Grid,'UD_index'))\n",
    "df_index_cols['LD_index'] = LD_index_values\n",
    "\n",
    "    \n",
    "# Create a list of columns for each domain type (components with domain subtext (e.g., LD) in the name)\n",
    "LD_components = [i for i in unique_components if \"LD\" in i]\n",
    "# print(\"LD components:\", LD_components)\n",
    "    # SD_components = [i for i in unique_components if \"SD\" in i]\n",
    "    # print(\"SD components:\", SD_components)\n",
    "    # domaintype = \"UD\"\n",
    "    # UD_components = [i for i in unique_components if domaintype in i]\n",
    "    # print(\"UD components:\", UD_components)\n",
    "\n",
    "### FOR DEVELOPMENT ONLY -- limit number of components\n",
    "# LD_components = LD_components[0:3]\n",
    "\n",
    "# Create DataFrame with values for records in each LD column\n",
    "col = {}\n",
    "for i in LD_components:\n",
    "    col[i] = FieldValues(PE_Grid, i)\n",
    "df_LD_fieldvalues = pd.DataFrame(col)\n",
    "\n",
    "\n",
    "# Join into a new DataFrame the LD_index and LD_components\n",
    "df_LD_joined = df_index_cols.join(df_LD_fieldvalues, sort=False)\n",
    "# print(df_LD_joined)\n",
    "\n",
    "# Group by unique LD_index values\n",
    "df_LD_grouped = df_LD_joined.groupby(['LD_index'])\n",
    "\n",
    "# Determine max of DA for each LD_index group, return in \"DA_...LD..._domain\" column\n",
    "df_LD_max = df_LD_grouped.max()\n",
    "for i in LD_components:\n",
    "    df_LD_max.rename(columns={i:(i + \"_domain\")}, inplace=True)\n",
    "df_LD_max.drop(['LG_index'], axis=1, inplace=True)  # LG_index is erroneously overwritten without this line\n",
    "# print(df_LD_max,\"\\n\")\n",
    "\n",
    "# Merge index and DA_max columns in a new DataFrame\n",
    "df_LD_export = df_index_cols.merge(df_LD_max, on='LD_index')\n",
    "# df_LD_all = df_LD_joined.merge(df_LD_max, on='LD_index')\n",
    "\n",
    "# Clear unnecessary variables from workspace\n",
    "# del(df_index_cols, df_LD_fieldvalues, df_LD_joined, df_LD_grouped, df_LD_max)\n",
    "\n",
    "# Export DataFrame as CSV\n",
    "exported_df = 'E:/REE/PE_Score_Calc/Development/10-10-19/LD_domains_exported.csv'\n",
    "df_LD_export.to_csv(exported_df, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\"\"\" Distribute DA across the different domains using a Pandas DataFrame\"\"\"\n",
    "\n",
    "### ONLY CONSIDERS LITHOLOGIC DOMAINS (LD): MAKING THIS INTO A FUNCTION THAT \n",
    "### TAKES THE DIFFERENT DOMAIN TYPES (LD, SD, SA)\n",
    "\n",
    "### CODE IN DEVELOPMENT - TESTED AND PARTIALLY SUCCESSFUL; BUT STILL NEED TO VALIDATE DOMAIN RESULTS... ###  \n",
    "\n",
    "### IMPLEMENTING AS A FUNCTION DOES NOT SEEM TO BE A GOOD METHOD SINCE ONLY ABLE TO HAVE SINGLE RETURN (CANNOT GET\n",
    "### iT TO EXPORT THE CSV FILE...) \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "######################################################################################################################\n",
    "def FieldValues(table, field):\n",
    "    \"\"\"\n",
    "    Create a list of unique values from a field in a feature class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    table: <str>\n",
    "        Name of the table or feature class\n",
    "        \n",
    "    field: <str>\n",
    "        Name of the field \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    unique_values: <list>\n",
    "        Field values \n",
    "    \"\"\"\n",
    "    # Create a cursor object for reading the table\n",
    "    cursor = arcpy.da.SearchCursor(table, [field]) # A cursor iterates over rows in table\n",
    "\n",
    "    # Create an empty list for unique values\n",
    "    unique_values = []\n",
    "\n",
    "    # Iterate through rows\n",
    "    for row in cursor:\n",
    "        unique_values.append(row[0])\n",
    "    \n",
    "    return unique_values\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# Set to zero the domain values for cells outside AOI (only need this for one instance; then turn off)\n",
    "# replaceNULL(PE_Grid,'LD_index')\n",
    "\n",
    "######################################################################################################################\n",
    "def calcDomain(domainType, workspace_dir):\n",
    "    \"\"\"\n",
    "    Calculate the presence/absesnce of a dataset within a geologic domain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    domainType: <str>\n",
    "        Name of the domain type.  Only the following two-letter strings should be used:\n",
    "        'LD' (lithologic domain)\n",
    "        'SD' (structural domain)\n",
    "        'UD' (unique domain)  # NOTE: revisit whether this will be used...\n",
    "        'SA' (secondary alteration)\n",
    "\n",
    "    workspace_dir: <str>\n",
    "        The working directory where the files will be exported.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_domains_export: <pandas dataframe>\n",
    "        A Pandas DataFrame containing the calculated fields to be incorporated into the PE_Grid feature class.\n",
    "        The input domain type is recored in the \n",
    "\n",
    "    *domainType*_domains_exported.csv <csv file>\n",
    "        The Pandas DataFrame exported as a csv file, with the input domain type recorded in the file name. \n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create a list of local grid index values, then create DataFrame\n",
    "    LG_index_values = FieldValues(PE_Grid,'LG_index')\n",
    "    # LD_values_unique = set(FieldValues(PE_Grid,'LD_index'))\n",
    "    df_index_cols = pd.DataFrame(LG_index_values, columns = {'LG_index'})\n",
    "\n",
    "    # Create a list of domain index values (e.g, LD1, LD2, LD3, LD4), then add to DataFrame\n",
    "    domainType_index_values = FieldValues(PE_Grid, domainType + '_index')\n",
    "    df_index_cols[domainType + '_index'] = domainType_index_values\n",
    "\n",
    "    # Create a list of columns for each domain type (components with domain subtext (e.g., LD) in the name)\n",
    "    domainType_components = [i for i in unique_components if domainType in i]\n",
    "\n",
    "    # Create DataFrame with values for records in each domainType column\n",
    "    col = {}\n",
    "    for i in domainType_components:\n",
    "        col[i] = FieldValues(PE_Grid, i)\n",
    "    df_domainType_fieldvalues = pd.DataFrame(col)\n",
    "\n",
    "    # Join into a new DataFrame the LD_index and domainType_components\n",
    "    df_domainType_joined = df_index_cols.join(df_domainType_fieldvalues, sort=False)\n",
    "\n",
    "    # Group by unique domainType_index values\n",
    "    df_domainType_grouped = df_domainType_joined.groupby([domainType + '_index'])\n",
    "\n",
    "    # Determine max of DA for each domainType_index group, return in \"DA_...domainType..._domain\" column\n",
    "    df_domainType_max = df_domainType_grouped.max()\n",
    "    for i in domainType_components:\n",
    "        df_domainType_max.rename(columns={i:(i + \"_domain\")}, inplace=True)\n",
    "    df_domainType_max.drop(['LG_index'], axis=1, inplace=True)  # LG_index is erroneously overwritten without this line\n",
    "\n",
    "    # Merge index and DA_max columns in a new DataFrame\n",
    "    df_domainType_export = df_index_cols.merge(df_domainType_max, on = domainType + '_index')\n",
    "    # df_domainType_all = df_domainType_joined.merge(df_=domainType_max, on = domainType + '_index')\n",
    "\n",
    "    # Export DataFrame as CSV\n",
    "    df_export_file = workspace_dir + domainType + r\"domains_exported.csv\"\n",
    "    export_file = df_domainType_export.to_csv(df_export_file, index=False)\n",
    "\n",
    "    return export_file\n",
    "\n",
    "### ######################################################################################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = list(df_domainType_export.columns)\n",
    "\n",
    "b = list(df_domainType_export.columns)\n",
    "\n",
    "field_List.remove(\"LG_index\")\n",
    "for domainType in domainTypes:\n",
    "    try:\n",
    "        fieldList.remove(str(domainType + \"_index\"))\n",
    "    except:\n",
    "        print()\n",
    "\n",
    "a\n",
    "# b"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# fieldList = list(df_domainType_export.columns[len(domainTypes)+1:])\n",
    "fieldList\n",
    "# domainType\n",
    "df_domainType_export['LG_index']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Export domainType DataFrame as CSV\n",
    "exported_df_domainType = workspace_dir + \"/\" + domainType + r\"_domains_exported.csv\"\n",
    "df_domainType_export.to_csv(exported_df_domainType, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = FieldValues(PE_Grid,'SD_index')\n",
    "print(a[-1])\n",
    "set(a)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(domainTypes)+1\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Add DataFrame fields to PE_Grid\"\"\"\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL; ...NEED TO VALIDATE DOMAIN RESULTS... ###  \n",
    "\n",
    "### CODE HAS BEEN INCORPORATED IN CELL ABOVE ###\n",
    "\n",
    "# Export DataFrame as CSV\n",
    "exported_df = 'E:/REE/PE_Score_Calc/Development/10-10-19/LD_domains_exported.csv'\n",
    "df_LD_export.to_csv(exported_df, index=False)\n",
    "\n",
    "# Convert the DataFrame CSV file to ArcGIS table (if not already created)\n",
    "try:\n",
    "    arcpy.TableToTable_conversion(exported_df, workspace, \"exported_df_table\")\n",
    "except:\n",
    "    print(\"DataFrame csv already converted to ArcGIS table!\")\n",
    "\n",
    "# Join DataFrame table to PE_Grid\n",
    "inFeatures = PE_Grid\n",
    "joinField = \"LG_index\"\n",
    "joinTable = \"exported_df_table\"\n",
    "fieldList = list(df_LD_export.columns[2:])\n",
    "arcpy.JoinField_management(inFeatures, joinField, joinTable, joinField, fieldList)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import and convert CSV file to ArcGIS table\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL ###\n",
    "\n",
    "### ADDED TO DATAFRAME CELL ###\n",
    "\n",
    "inTable = exported_df\n",
    "outLocation = workspace\n",
    "outTable = \"exported_df_table\"\n",
    "arcpy.TableToTable_conversion(inTable, outLocation, outTable)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Join exported DataFrame to PE_Grid\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL ###\n",
    "\n",
    "### ADDED TO DATAFRAME CELL ###\n",
    "\n",
    "inFeatures = PE_Grid\n",
    "joinField = \"LG_index\"\n",
    "joinTable = outTable\n",
    "fieldList = list(df_LD_export.columns[2:])\n",
    "# fieldList = 'DA_Eo_LD_CID10_domain'\n",
    "\n",
    "arcpy.JoinField_management(inFeatures, joinField, joinTable, joinField, fieldList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRATCH CELLS BELOW"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# len(LD_values)\n",
    "# print(df_LD_all.columns)\n",
    "df_LD_export"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Update field names\n",
    "field_names = ListFieldNames(PE_Grid)\n",
    "print(field_names)\n",
    "\n",
    "# cull = list(df_LD_export.columns[2:4])\n",
    "\n",
    "# for domain in cull:\n",
    "#     arcpy.DeleteField_management(PE_Grid, domain)\n",
    "    \n",
    "# field_names = ListFieldNames(PE_Grid)\n",
    "# print(field_names)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\"\"\" Add to PE_Grid new fields for LD_domain calculations  \"\"\"\n",
    "\n",
    "### STILL IN DEVELOPMENT ###\n",
    "\n",
    "### SINGLE COLUMN ONLY -- NEED TO ITERATE THROUGH ALL FIELDS THAT END IN _domain ###\n",
    "\n",
    "### ALREADY ADDED _domain FIELDS TO PE_GRID ####\n",
    "\n",
    "domain_fields = []\n",
    "for i in LD_components:\n",
    "    domain_fields = domain_fields + [i + str(\"_domain\")]\n",
    "#     arcpy.AddField_management(PE_Grid, domain_fields, \"SHORT\")\n",
    "print(domain_fields[0])\n",
    "\n",
    "# Update values for _domain field\n",
    "counter = -1\n",
    "with arcpy.da.UpdateCursor(PE_Grid, domain_fields[0]) as cursor:\n",
    "    for row in cursor:\n",
    "        counter = counter + 1\n",
    "        if counter > len(LD_values):\n",
    "            counter = 0\n",
    "        row[0] = df[domain_fields[0]][counter]\n",
    "        cursor.updateRow(row)  # Update the cursor with the updated list\n",
    "        \n",
    "# Update values for _domain field\n",
    "# counter = -1\n",
    "# for LD in domain_fields:\n",
    "#     with arcpy.da.UpdateCursor(PE_Grid, LD) as cursor:\n",
    "#         for row in cursor:\n",
    "#             counter = counter + 1\n",
    "#             if counter > len(LD_values):\n",
    "#                 counter = 0\n",
    "#             row[0] = df[LD][counter]\n",
    "#             cursor.updateRow(row)  # Update the cursor with the updated list\n",
    "        \n",
    "### DEVELOPMENT ONLY -- VERIFY FIELD VALUES ARE CORRECT ###\n",
    "FieldValues(PE_Grid, domain_fields[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(LD)\n",
    "print(domain_fields[0])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "counter = 0\n",
    "LD = domain_fields[0]\n",
    "df[LD][0]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "\"\"\" Calculate DA_sum column in new DataFrame column \"\"\"\n",
    "\n",
    "### CODE TESTED AND SUCCESSFUL ###\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create test list for LD_index\n",
    "list1 = {'LD_index': ['LD1']*5 + ['LD2']*5, 'DA': [0,1,0,0,1,0,0,1,1,1]}\n",
    "# print(list1)\n",
    "\n",
    "# Create Pandas DataFrame\n",
    "df = pd.DataFrame(list1)\n",
    "print(df,\"\\n\")\n",
    "\n",
    "# Group by unique LD_index values\n",
    "LD = df.groupby(['LD_index'])\n",
    "\n",
    "# Determine sum of DA for each LD_index group, return in \"DA_domain\" column\n",
    "LD_max = LD.max()\n",
    "LD_max.rename(columns={\"DA\":\"DA_domain\"}, inplace=True)\n",
    "print(LD_max,\"\\n\")\n",
    "\n",
    "# Merge DA_sum column to original DataFrame\n",
    "df = df.merge(LD_max,on='LD_index')\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### CODE NOT NEEDED ### \n",
    "\n",
    "# create a function for domain distribution\n",
    "\n",
    "# for each record:\n",
    "    # if DA component == 1, record +1 for that domain (e.g., LD1 = LD1 + 1)\n",
    "\n",
    "# # create empty list for counter \n",
    "# LD_counter = []\n",
    "# for i in LD_values:\n",
    "#     LD_counter.append(0)\n",
    "    \n",
    "\n",
    "# import itertools\n",
    "# for ldc in LD_components:\n",
    "#     for ldv in LD_values, range(len(LD_values)):\n",
    "#         if LD_index == ldv:\n",
    "#             LD_counter = LD_counter + ldc\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LD_counter = []\n",
    "for i in LD_values:\n",
    "    LD_counter.append(0)\n",
    "print(LD_counter)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LD_list = [0] * len(records)\n",
    "\n",
    "SearchCursor(PE_Grid, LD_components)\n",
    "for i in LD_list:\n",
    "    if i = LD_index:\n",
    "        LD_list[i] = LD_list[i] + sum[row]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
